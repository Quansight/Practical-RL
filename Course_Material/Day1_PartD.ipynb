{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec9084c-bb67-4703-bc93-a22894fcd88b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Day 1, Part D: Parking a Car with RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ced8cf-099a-4399-8abd-61b10efedd03",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "- Gain more experience with a different model and environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7b25f-c42d-4899-956b-5e6980dfcda6",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "- **Simulation environment**: Notice that this is not the same as the python/conda environment.  The simulation environment is the simulated world where the reinforcement learning takes place.  It provides opportunities for an agent to learn and explore, and ideally provides challenges that aid in efficient learning.\n",
    "- **Agent (aka actor or policy)**: An entity in the simulation environment that performs actions.  The agent could be a person, a robot, a car, a thermostat, etc.\n",
    "- **State variable**: An observed variable in the simulation environment.  They can be coordinates of objects or entities, an amount of fuel in a tank, air temperature, wind speed, etc.\n",
    "- **Action variable**: An action that the agent can perform.  Examples: step forward, increase velocity to 552.5 knots, push object left with force of 212.3 N, etc.\n",
    "- **Reward**: A value given to the agent for doing something considered to be 'good'.  Reward is commonly assigned at each time step and cumulated during a learning episode.\n",
    "- **Episode**: A learning event consisting of multiple steps in which the agent can explore.  It starts with the unmodified environment and continues until the goal is achieved or something prevents further progress, such as a robot getting stuck in a hole.  Multiple episodes are typically run in loops until the model is fully trained.\n",
    "- **Model (aka policy or agent)**: An RL model is composed of the modeling architecture (e.g., neural network) and parameters or weights that define the unique behavior of the model.\n",
    "- **Policy (aka model or agent)**: The parameters of a model that encode the best choices to make in an environment.  The choices are not necessarily good ones until the model undergoes training.  The policy (or model) is the \"brain\" of the agent.\n",
    "- **Replay Buffer**: A place in memory to store state, action, reward and other variables describing environmental state transitions. It is effectively the agent's memory of past experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1526bd5-ae3e-4bb3-af45-036178cc92a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Reinforcement Learning Cycle](./images/Reinforcement-learning-diagram-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f776640c-afed-407a-b829-190e32f6be16",
   "metadata": {},
   "source": [
    "## Parking Environment\n",
    "\n",
    "Here's another RL challenge that will take several hours to run.  Try running this overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b4f83-6fb8-45dc-8fc0-33c6480cf354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Image(\"images/parking-env.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9dffe7-9ae2-4f7d-9171-1658df0a2e42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import HerReplayBuffer, SAC, DDPG, TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "\n",
    "env = gym.make(\"parking-v0\")\n",
    "\n",
    "# Create 4 artificial transitions per real transition\n",
    "n_sampled_goal = 4\n",
    "\n",
    "# SAC hyperparams:\n",
    "model = SAC(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    replay_buffer_class=HerReplayBuffer,\n",
    "    replay_buffer_kwargs=dict(\n",
    "      n_sampled_goal=n_sampled_goal,\n",
    "      goal_selection_strategy=\"future\",\n",
    "      # IMPORTANT: because the env is not wrapped with a TimeLimit wrapper\n",
    "      # we have to manually specify the max number of steps per episode\n",
    "      max_episode_length=100,\n",
    "      online_sampling=True,\n",
    "    ),\n",
    "    verbose=1,\n",
    "    buffer_size=int(1e6),\n",
    "    learning_rate=1e-3,\n",
    "    gamma=0.95,\n",
    "    batch_size=256,\n",
    "    policy_kwargs=dict(net_arch=[256, 256, 256]),\n",
    ")\n",
    "\n",
    "model.learn(int(2e5))\n",
    "model.save(\"her_sac_highway\")\n",
    "\n",
    "# Load saved model\n",
    "# Because it needs access to `env.compute_reward()`\n",
    "# HER must be loaded with the env\n",
    "model = SAC.load(\"her_sac_highway\", env=env)\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "# Evaluate the agent\n",
    "episode_reward = 0\n",
    "for _ in range(100):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    episode_reward += reward\n",
    "    if done or info.get(\"is_success\", False):\n",
    "        print(\"Reward:\", episode_reward, \"Success?\", info.get(\"is_success\", False))\n",
    "        episode_reward = 0.0\n",
    "        obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505ab94-3111-4bd6-8e44-310f000bc59b",
   "metadata": {},
   "source": [
    "Remember that the rendered model (image window) can be closed by restarting the kernel or shutting down the notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
