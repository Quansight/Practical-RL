{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c88cdd",
   "metadata": {},
   "source": [
    "## Day 5, Part B: Using Your Trained Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a60c5f-3997-4d03-9c6e-f4ded3ab6778",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "- What to do with the trained policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6f2bf-7a6f-4a89-9510-b998bfc602b1",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "- **Simulation environment**: Notice that this is not the same as the python/conda environment.  The simulation environment is the simulated world where the reinforcement learning takes place.  It provides opportunities for an agent to learn and explore, and ideally provides challenges that aid in efficient learning.\n",
    "- **Agent (aka actor or policy)**: An entity in the simulation environment that performs actions.  The agent could be a person, a robot, a car, a thermostat, etc.\n",
    "- **State variable**: An observed variable in the simulation environment.  They can be coordinates of objects or entities, an amount of fuel in a tank, air temperature, wind speed, etc.\n",
    "- **Action variable**: An action that the agent can perform.  Examples: step forward, increase velocity to 552.5 knots, push object left with force of 212.3 N, etc.\n",
    "- **Reward**: A value given to the agent for doing something considered to be 'good'.  Reward is commonly assigned at each time step and cumulated during a learning episode.\n",
    "- **Episode**: A learning event consisting of multiple steps in which the agent can explore.  It starts with the unmodified environment and continues until the goal is achieved or something prevents further progress, such as a robot getting stuck in a hole.  Multiple episodes are typically run in loops until the model is fully trained.\n",
    "- **Model (aka policy or agent)**: An RL model is composed of the modeling architecture (e.g., neural network) and parameters or weights that define the unique behavior of the model.\n",
    "- **Policy (aka model or agent)**: The parameters of a model that encode the best choices to make in an environment.  The choices are not necessarily good ones until the model undergoes training.  The policy (or model) is the \"brain\" of the agent.\n",
    "- **Replay Buffer**: A place in memory to store state, action, reward and other variables describing environmental state transitions. It is effectively the agent's memory of past experiences.\n",
    "- **On-policy**: The value of the next action is determined using the current actor policy.\n",
    "- **Off-policy**: The value of the next action is determined by a function, such as a value function, instead of the current actor policy.\n",
    "- **Value function**: Function (typically a neural network) used to estimate the value, or expected reward, of an action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54628380-9950-4d9b-89f0-11feb254ce58",
   "metadata": {},
   "source": [
    "## I've trained. I'm happy with the results. Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c76ad9-5c5d-48a4-9ad3-329eb5be614b",
   "metadata": {},
   "source": [
    "Training a RL policy can be very time consuming and expensive, so you want to make sure it's put to good use.  Before we try to make use of the trained model, let's be sure it's ready to use.  In previous notebooks, we have been saving the models (e.g., Day 1 Part C), but there are additional nuances that can be helpful during training.\n",
    "\n",
    "- Consider auto-saving the highest reward policy\n",
    "\n",
    "Check your training algorithm before you fire things up: does it auto-save the highest-reward policy? Does it have mechanisms built in to allow you to pick up where you left off, if things need to be paused, or temporarily brought completely offline?  A lot of the frameworks have these things included, but you want to verify and put those things in by hand (or add more for your own interests) should you need them. \n",
    "\n",
    "Maybe you need to stand up a database to store a large number of policy snapshots.  Maybe you want to keep track of the model buffer states and keep that in storage as well. But there's a golden rule you may have learned from late nights writing school essays and the power goes out: \"Save. And save often.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ced074-02e8-4b1f-ab8c-26bfcacdad7e",
   "metadata": {},
   "source": [
    "From our original stable_baselines3 CartPole, that looked like:\n",
    "\n",
    "`model.save(\"ppo_cartpole\")`\n",
    "\n",
    "In that case, the line produced a `ppo_cartpole.zip` file; others might produce a numpy `.npy`, or simply an 'object'.  In any case, that's the artifact that you spent all your time and money producing - the stored values in the neural network.\n",
    "\n",
    "If you still have the ppo_cartpole.zip around, we can load the thing up and put it to use; otherwise, rerun Day1_PartC and create the thing.\n",
    "\n",
    "Now, we import the same boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e243c3-2b4d-4b12-b481-d1069e40079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2317e-694c-44ba-b0b8-621e5b9cef55",
   "metadata": {},
   "source": [
    "Create our environment from `gym.make()` and load the zip back in to a variable using stable-baselines3's PPO load utilty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c38d5db-87bf-45a6-b885-b269b5acd787",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "model = PPO.load(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f17c349-cddb-45c6-8fa9-f1440f7980bc",
   "metadata": {},
   "source": [
    "We'll go ahead and do the same render we did before to 'see it in action', but lets take a look at what we have already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49179f78-477d-486f-97d8-8df181ae5711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01275246, -0.03625066, -0.01694884,  0.03791228])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d119a42-99da-4928-bd17-815f4004dd68",
   "metadata": {},
   "source": [
    "This array - that's the environment state at reset.  Feel free to re-run the above cell a few times, you'll see different results for each run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b5468-899e-40a1-b31e-50192ea14560",
   "metadata": {},
   "source": [
    "Passing that environment state to the policy in `model.predict(obs)` returns the policy action to take given the current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7618c10-54cd-47ea-87a8-a6a989c1115e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action, _states = model.predict(obs)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b051ef-e6c1-4047-9bf0-480bff0003cd",
   "metadata": {},
   "source": [
    "Believe it or not, that's the *entire* ballgame. All that time and effort gives you a policy that delivers one thing: given my environment state, what action do I take?\n",
    "\n",
    "You *can* hook things up to run an entire episode and play things out like a simulation/game/etc, or you could just take that single one-off state->action converter and drop it in to another piece of code.  Maybe you have a theory-based algorithm that solves your problem perfectly, outside of just that one blind point where your function has a divide-by-zero (shrug), and in that exception catch you drop in your trained policy to do a bit more than just a simple 'default action.'\n",
    "\n",
    "You can run the next two cells, again and again, to advance the environment forward; predict->step->predict->step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b9ea0ea-22e6-46e3-8120-2c5648eca301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.0098824 , -0.42540461, -0.01457632,  0.59935803]), 1.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rewards, dones, info = env.step(action)\n",
    "obs, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc7379f-2274-4772-8bb4-abeca48d2ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action, _states = model.predict(obs)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f08998-975d-47c8-9832-b85827d85f16",
   "metadata": {},
   "source": [
    "We can, of course, just run the thing through the episode (or 1k steps here below) given our policy.  But we just want to remind you, you don't have to do just that with the policy you've trained.  Your policy is now a function that pops out 'best actions' when you ask it to - set up a multiplayer game, for example, and every time the computer gets a turn (or opportunity to move in some way.. maybe on a set polling interval, maybe 0.05s) you pass the observations to your policy, and there's your AI player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f62a0-98e7-4af9-857a-19515d4dc2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "env.env.viewer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c5501-7902-4ea1-903f-437a64611fa9",
   "metadata": {},
   "source": [
    "For the case of TD3, there are save and load functions built in, and they look like this:\n",
    "\n",
    "```\n",
    "    def save(self, filename):\n",
    "        torch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "        torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "\n",
    "        torch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "        torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "        self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "\n",
    "        self.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "        self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "```\n",
    "\n",
    "It's simply using the PyTorch functions `torch.save()` and `torch.load()` to load the NN objects for actor and critic - then, when we go to take similar steps to those above, we're now asking a loaded policy: `policy.select_action(np.array(state))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66365149-7e32-4c47-9c05-956be5574ffe",
   "metadata": {},
   "source": [
    "There's a lot of code in the next two cells, but it is rather simple in what it's doing: boilerplate imports/loads and environment register; then a `load_policy()` that is nearly identical to the first half of the `main()` function we were playing with in our Ant examples.  It just stops as soon as it has the TD3 load accomplished, with correct parameters.  We don't need most of them, but we're bringing them along for the ride, just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7470d00d-698f-4a47-8544-7c7590e7eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "import utils\n",
    "import TD3\n",
    "from numpngw import write_apng\n",
    "from gym.envs.registration import registry, make, spec\n",
    "\n",
    "def register(id, *args, **kvargs):\n",
    "  if id in registry.env_specs:\n",
    "    return\n",
    "  else:\n",
    "    return gym.envs.registration.register(id, *args, **kvargs)\n",
    "\n",
    "register(id='MyAntBulletEnv-v0',\n",
    "         entry_point='override_ant_random:MyAntBulletEnv',\n",
    "         max_episode_steps=3000,\n",
    "         reward_threshold=2500.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e7f874c-d9e0-4ed8-9fc3-648df25fe294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_policy(env_name_var):\n",
    "    args = {\n",
    "            \"policy\" : \"TD3\",                  # Policy name (TD3, DDPG or OurDDPG)\n",
    "            \"env\" : env_name_var,              # OpenAI gym environment name\n",
    "            \"seed\" : 0,                        # Sets Gym, PyTorch and Numpy seeds\n",
    "            \"start_timesteps\" : 25e3,          # Time steps initial random policy is used\n",
    "            \"eval_freq\" : 5e3,                 # How often (time steps) we evaluate\n",
    "            \"max_timesteps\" : 2e6,             # Max time steps to run environment\n",
    "            \"expl_noise\" : 0.1,                # Std of Gaussian exploration noise\n",
    "            \"batch_size\" : 256,                # Batch size for both actor and critic\n",
    "            \"discount\" : 0.99,                 # Discount factor\n",
    "            \"tau\" : 0.007,                     # Target network update rate\n",
    "            \"policy_noise\" : 0.2,              # Noise added to target policy during critic update\n",
    "            \"noise_clip\" : 0.5,                # Range to clip target policy noise\n",
    "            \"policy_freq\" : 2,                 # Frequency of delayed policy updates\n",
    "            \"save_model\" : \"store_true\",       # Save model and optimizer parameters\n",
    "            \"load_model\" : \"default\",           # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "           }\n",
    "\n",
    "    file_name = f\"{args['policy']}_{args['env']}_{args['seed']}_{args['tau']}\"\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Policy: {args['policy']}, Env: {args['env']}, Seed: {args['seed']}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    if not os.path.exists(\"./results\"):\n",
    "        os.makedirs(\"./results\")\n",
    "\n",
    "    if args['save_model'] and not os.path.exists(\"./models\"):\n",
    "        os.makedirs(\"./models\")\n",
    "\n",
    "    env = gym.make(args['env'])\n",
    "\n",
    "    # Set seeds\n",
    "    env.seed(args['seed'])\n",
    "    env.action_space.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0] \n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    kwargs = {\n",
    "        \"state_dim\": state_dim,\n",
    "        \"action_dim\": action_dim,\n",
    "        \"max_action\": max_action,\n",
    "        \"discount\": args['discount'],\n",
    "        \"tau\": args['tau'],\n",
    "    }\n",
    "\n",
    "    # Initialize policy\n",
    "    if args['policy'] == \"TD3\":\n",
    "        # Target policy smoothing is scaled wrt the action scale\n",
    "        kwargs[\"policy_noise\"] = args['policy_noise'] * max_action\n",
    "        kwargs[\"noise_clip\"] = args['noise_clip'] * max_action\n",
    "        kwargs[\"policy_freq\"] = args['policy_freq']\n",
    "        policy = TD3.TD3(**kwargs)\n",
    "\n",
    "    if args['load_model'] != \"\":\n",
    "        policy_file = file_name if args['load_model'] == \"default\" else args['load_model']\n",
    "        policy.load(f\"./models/{policy_file}\")\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6253965b-d630-41ca-9543-d3764d6188ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Policy: TD3, Env: MyAntBulletEnv-v0, Seed: 0\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Oct 11 2021 21:00:24\n",
      "/home/eric/miniconda3/envs/TD3/lib/python3.9/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "policy = load_policy(\"MyAntBulletEnv-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64c8037f-f28a-4c14-b9fa-f46ff0639e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MyAntBulletEnv-v0\", render=True)\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5690c03e-11bd-4c41-8693-fd6dd6ae1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, done = env.reset(), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aef0f18c-8b27-400f-8cf4-f8a89907fa00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.7071182 ,  0.7070953 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.12766643,  0.        ,\n",
       "       -1.7047719 ,  0.        ,  0.03801315,  0.        ,  1.7883936 ,\n",
       "        0.        , -0.11387987,  0.        ,  1.9138088 ,  0.        ,\n",
       "       -0.03077449,  0.        , -1.8015345 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759966cc-d37a-4585-9c09-f94b2a1dbfa3",
   "metadata": {},
   "source": [
    "Now everything's set up, you can view the ant and step through the simulation with the next cell (we could have even just made that it's own function - call it 'advance' or something).  Control-enter will run cell and stay on the same cell.\n",
    "\n",
    "At this point, it's fun to keep the sim window and the notebook both visible (I shrink my notebook to see the window on the side).  The view can be adjusted with control-click, mouse wheel, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e1b8109a-65da-4f32-ab12-7e6e1e2c7507",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = policy.select_action(np.array(state))\n",
    "state, reward, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "738c38f7-0888-46c6-bbdd-27c4026afd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0878694444214176, 0.9366856447023537, 0.5758033039290331)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.robot.body_xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e2a58-37e7-479b-a64d-9073179f8d4b",
   "metadata": {},
   "source": [
    "Maybe mid-course we want to change the target the ant is walking to (which is then in the obs space):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1c7bbeba-85f8-4c8e-be27-e6cb262a51fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.robot.walk_target_x = -10\n",
    "env.robot.walk_target_y = -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6ff85-bbe2-404d-a431-16791861b6c0",
   "metadata": {},
   "source": [
    "And more re-running the advance cell...\n",
    "\n",
    "You get the picture - the policy controls what actions are taken, it's an artifact that we save and load, but other than that it's up to us how it gets used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da606f-d032-41b2-b6e2-3f2ee6cb8d6a",
   "metadata": {},
   "source": [
    "For fun - lets set it up so we can pass it a list of points the ant needs to walk through and we can pass in walking-path coordinates.  Maybe this list could be provided by another path-finding AI, or classical control scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c7f195d6-325e-409d-92c1-ee8f8f2114c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [(3,3),(0,3),(-6,-6),(0,-6),(9,9),(0,9),(0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "1dbdb515-dbc1-4bcb-a38b-e5bf2f71ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in my_list:\n",
    "    env.robot.walk_target_x = i[0]\n",
    "    env.robot.walk_target_y = i[1]\n",
    "    path_done = False\n",
    "    counter_i = 0\n",
    "    while not path_done:\n",
    "        action = policy.select_action(np.array(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        time.sleep(1. / 100.) #comment out to run at max local system speed\n",
    "        counter_i += 1\n",
    "        if counter_i > 500:\n",
    "            path_done = True\n",
    "            \n",
    "        dist = np.linalg.norm([i[0]-env.robot.body_xyz[0],i[1]-env.robot.body_xyz[1]])\n",
    "        if dist < 0.2:\n",
    "            path_done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd0e5d5-03fc-4b53-b403-f2432e8ccf7a",
   "metadata": {},
   "source": [
    "Don't let all that power go to your head.. poor little ant.\n",
    "\n",
    "Try changing the list up a few times and see the ant run different routes.  This particular policy is from the 5m time-step custom ant with no reward modification - it will mostly get the job done, but there will be a few instances where it just doesn't make the next point happen (why we give it the 500 time-step counter time-out).  For fun, try taking one of your modified ants with custom reward and send it through a similar challenge.  \n",
    "\n",
    "What happens if you (by-hand) manually tweak some of the robot internal state values as its moving?  \n",
    "Is the ant robust to observation signal noise?  How might you modify the training course so it would handle real-world sensor noise/errors/corruptions that it might encounter if the policy were place into a real robot ant?  \n",
    "What modifications might you want to make to the environment that the ant is trained in?  \n",
    "The observables it sees - would they be general enough to handle the 'real world'?\n",
    "\n",
    "There are lots of things to consider and weigh when building out your RL environment, training, and how you use the policy - but hopefully by this point you can start to answer some of these questions and think about what you might do, yourself.  Best of luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1783fb-cd20-4685-88ac-8e3a60d97ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
