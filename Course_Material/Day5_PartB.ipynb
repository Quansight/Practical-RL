{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c88cdd",
   "metadata": {},
   "source": [
    "## Day 5, Part B: Using Your Trained Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a60c5f-3997-4d03-9c6e-f4ded3ab6778",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "- What to do with the trained policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6f2bf-7a6f-4a89-9510-b998bfc602b1",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "- **Simulation environment**: Notice that this is not the same as the python/conda environment.  The simulation environment is the simulated world where the reinforcement learning takes place.  It provides opportunities for an agent to learn and explore, and ideally provides challenges that aid in efficient learning.\n",
    "- **Agent (aka actor or policy)**: An entity in the simulation environment that performs actions.  The agent could be a person, a robot, a car, a thermostat, etc.\n",
    "- **State variable**: An observed variable in the simulation environment.  They can be coordinates of objects or entities, an amount of fuel in a tank, air temperature, wind speed, etc.\n",
    "- **Action variable**: An action that the agent can perform.  Examples: step forward, increase velocity to 552.5 knots, push object left with force of 212.3 N, etc.\n",
    "- **Reward**: A value given to the agent for doing something considered to be 'good'.  Reward is commonly assigned at each time step and cumulated during a learning episode.\n",
    "- **Episode**: A learning event consisting of multiple steps in which the agent can explore.  It starts with the unmodified environment and continues until the goal is achieved or something prevents further progress, such as a robot getting stuck in a hole.  Multiple episodes are typically run in loops until the model is fully trained.\n",
    "- **Model (aka policy or agent)**: An RL model is composed of the modeling architecture (e.g., neural network) and parameters or weights that define the unique behavior of the model.\n",
    "- **Policy (aka model or agent)**: The parameters of a model that encode the best choices to make in an environment.  The choices are not necessarily good ones until the model undergoes training.  The policy (or model) is the \"brain\" of the agent.\n",
    "- **Replay Buffer**: A place in memory to store state, action, reward and other variables describing environmental state transitions. It is effectively the agent's memory of past experiences.\n",
    "- **On-policy**: The value of the next action is determined using the current actor policy.\n",
    "- **Off-policy**: The value of the next action is determined by a function, such as a value function, instead of the current actor policy.\n",
    "- **Value function**: Function (typically a neural network) used to estimate the value, or expected reward, of an action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54628380-9950-4d9b-89f0-11feb254ce58",
   "metadata": {},
   "source": [
    "## I've trained. I'm happy with the results. Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c76ad9-5c5d-48a4-9ad3-329eb5be614b",
   "metadata": {},
   "source": [
    "Training a RL policy can be very time consuming and expensive, so you want to make sure it's put to good use.  Before we try to make use of the trained model, let's be sure it's ready to use.  In previous notebooks, we have been saving the models (e.g., Day 1 Part C), but there are additional nuances that can be helpful during training.\n",
    "\n",
    "- Consider auto-saving the highest reward policy during training\n",
    "- Consider auto-saving periodically in case you need to pause a long training run or there is a power outage\n",
    "\n",
    "A lot of the frameworks have these things included, but you want to verify or put those things in by hand (or add more for your own interests) should you need them. TD3 has some, and we'll mention those below.\n",
    "\n",
    "You might also want to stand up a database to store a large number of policy snapshots or to keep the model buffer state-action transitions (Methods that learn from previously determined transitions are a new field in RL). \n",
    "\n",
    ">**There's a golden rule you may have learned from late nights writing school essays and the power goes out: \"Save. And save often.\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ced074-02e8-4b1f-ab8c-26bfcacdad7e",
   "metadata": {},
   "source": [
    "From our original stable_baselines3 CartPole:\n",
    "\n",
    "```python\n",
    "model.save(\"ppo_cartpole\")\n",
    "```\n",
    "\n",
    "In this case, saving produced a `ppo_cartpole.zip` file; others might produce a NumPy `.npy`, or simply an 'object'.  These always contain the model values, but some also include other aspects of training, like episode number, so you can restart training where you left off.  That depends on the library you use.  In any case, **the file is the artifact that you spent all your time and money producing - the stored values in the neural network**.\n",
    "\n",
    "If you still have the ppo_cartpole.zip around, you can load it up and put it to use; otherwise, rerun Day1_PartC and create it.\n",
    "\n",
    "Now, import the same boilerplate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e243c3-2b4d-4b12-b481-d1069e40079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2317e-694c-44ba-b0b8-621e5b9cef55",
   "metadata": {},
   "source": [
    "Create our environment from `gym.make()` and load the zip back in to a variable using stable-baselines3's PPO load utilty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c38d5db-87bf-45a6-b885-b269b5acd787",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "model = PPO.load(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f17c349-cddb-45c6-8fa9-f1440f7980bc",
   "metadata": {},
   "source": [
    "We'll go ahead and do the same render we did before to 'see it in action', but lets take a look at what we have already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49179f78-477d-486f-97d8-8df181ae5711",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d119a42-99da-4928-bd17-815f4004dd68",
   "metadata": {},
   "source": [
    "This array is the environment state at reset.  Feel free to re-run the above cell a few times, you'll see different results for each run.  (Use control-enter to re-run without leaving the cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b5468-899e-40a1-b31e-50192ea14560",
   "metadata": {},
   "source": [
    "Passing that environment state to the policy in `model.predict(obs)` returns the policy action to take given the current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7618c10-54cd-47ea-87a8-a6a989c1115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "action, _states = model.predict(obs)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b051ef-e6c1-4047-9bf0-480bff0003cd",
   "metadata": {},
   "source": [
    "Believe it or not, for CartPole, that's the *entire* ballgame. All that time and effort gives you a policy that delivers one thing: the action to take given the state of the environment.\n",
    "\n",
    ">**Remember that you are building a tool.**\n",
    "\n",
    "You *can* hook things up to run an entire episode and play things out like a simulation/game/etc., or you could just take that single one-off state->action converter and drop it into another piece of code.  Maybe you have a theory-based algorithm that solves your problem perfectly, except for that one blind point where your algorithm has a divide-by-zero (shrug), so in that exception catch you drop in your trained policy to do a bit more than just a simple 'default action.'\n",
    "\n",
    ">**Try running the next two cells, again and again, to advance the environment forward; predict->step->predict->step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ea0ea-22e6-46e3-8120-2c5648eca301",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, dones, info = env.step(action)\n",
    "obs, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc7379f-2274-4772-8bb4-abeca48d2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "action, _states = model.predict(obs)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f08998-975d-47c8-9832-b85827d85f16",
   "metadata": {},
   "source": [
    "We can, of course, just run the thing through the episode (or 1k steps in a loop below) given our policy.  But we just want to remind you, you don't have to do just that with the policy you've trained.  \n",
    "\n",
    ">**Your policy is now a function that pops out 'best actions' when you ask it to**\n",
    "\n",
    "Set up a multiplayer game, for example, and every time the computer gets a turn (or opportunity to move in some way... maybe on a set polling interval, or maybe 0.05 seconds, or whatever) you pass the observations to your policy, and your AI player can act."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f62a0-98e7-4af9-857a-19515d4dc2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "env.env.viewer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c5501-7902-4ea1-903f-437a64611fa9",
   "metadata": {},
   "source": [
    "For the case of TD3, there are save and load functions built in, and they look like this:\n",
    "\n",
    "```python\n",
    "    def save(self, filename):\n",
    "        torch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "        torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "\n",
    "        torch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "        torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "        self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "\n",
    "        self.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "        self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "```\n",
    "\n",
    "It's simply using the PyTorch functions `torch.save()` and `torch.load()` to load the objects for actor and critic - then, when we request action and state updates, we're now asking a loaded (trained or partially trained) policy: `policy.select_action(np.array(state))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66365149-7e32-4c47-9c05-956be5574ffe",
   "metadata": {},
   "source": [
    "## Load Ant and have it perform some actions\n",
    "\n",
    "There's a lot of code in the next two cells, but it is rather simple in what it's doing: \n",
    "- import/load boilerplate\n",
    "- register the environment\n",
    "- load the policy\n",
    "\n",
    "The `load_policy()` in this case is nearly identical to the first half of the `main()` function we were playing with in our Ant examples.  It just stops as soon as it has the TD3 load accomplished, with correct parameters.  We don't need most of them, but we're bringing them along for the ride, just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470d00d-698f-4a47-8544-7c7590e7eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "import utils\n",
    "import TD3\n",
    "from numpngw import write_apng\n",
    "from gym.envs.registration import registry, make, spec\n",
    "\n",
    "def register(id, *args, **kvargs):\n",
    "    if id in registry.env_specs:\n",
    "        return\n",
    "    else:\n",
    "        return gym.envs.registration.register(id, *args, **kvargs)\n",
    "\n",
    "register(id='MyAntBulletEnv-v0',\n",
    "         entry_point='override_ant_random:MyAntBulletEnv',\n",
    "         max_episode_steps=3000,\n",
    "         reward_threshold=2500.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f874c-d9e0-4ed8-9fc3-648df25fe294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_policy(env_name_var):\n",
    "    args = {\n",
    "            \"policy\" : \"TD3\",                  # Policy name (TD3, DDPG or OurDDPG)\n",
    "            \"env\" : env_name_var,              # OpenAI gym environment name\n",
    "            \"seed\" : 0,                        # Sets Gym, PyTorch and Numpy seeds\n",
    "            \"start_timesteps\" : 25e3,          # Time steps initial random policy is used\n",
    "            \"eval_freq\" : 5e3,                 # How often (time steps) we evaluate\n",
    "            \"max_timesteps\" : 2e6,             # Max time steps to run environment\n",
    "            \"expl_noise\" : 0.1,                # Std of Gaussian exploration noise\n",
    "            \"batch_size\" : 256,                # Batch size for both actor and critic\n",
    "            \"discount\" : 0.99,                 # Discount factor\n",
    "            \"tau\" : 0.007,                     # Target network update rate\n",
    "            \"policy_noise\" : 0.2,              # Noise added to target policy during critic update\n",
    "            \"noise_clip\" : 0.5,                # Range to clip target policy noise\n",
    "            \"policy_freq\" : 2,                 # Frequency of delayed policy updates\n",
    "            \"save_model\" : \"store_true\",       # Save model and optimizer parameters\n",
    "            \"load_model\" : \"default\",           # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "           }\n",
    "\n",
    "    file_name = f\"{args['policy']}_{args['env']}_{args['seed']}_{args['tau']}\"\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Policy: {args['policy']}, Env: {args['env']}, Seed: {args['seed']}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    if not os.path.exists(\"./results\"):\n",
    "        os.makedirs(\"./results\")\n",
    "\n",
    "    if args['save_model'] and not os.path.exists(\"./models\"):\n",
    "        os.makedirs(\"./models\")\n",
    "\n",
    "    env = gym.make(args['env'])\n",
    "\n",
    "    # Set seeds\n",
    "    env.seed(args['seed'])\n",
    "    env.action_space.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0] \n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    kwargs = {\n",
    "        \"state_dim\": state_dim,\n",
    "        \"action_dim\": action_dim,\n",
    "        \"max_action\": max_action,\n",
    "        \"discount\": args['discount'],\n",
    "        \"tau\": args['tau'],\n",
    "    }\n",
    "\n",
    "    # Initialize policy\n",
    "    if args['policy'] == \"TD3\":\n",
    "        # Target policy smoothing is scaled wrt the action scale\n",
    "        kwargs[\"policy_noise\"] = args['policy_noise'] * max_action\n",
    "        kwargs[\"noise_clip\"] = args['noise_clip'] * max_action\n",
    "        kwargs[\"policy_freq\"] = args['policy_freq']\n",
    "        policy = TD3.TD3(**kwargs)\n",
    "\n",
    "    if args['load_model'] != \"\":\n",
    "        policy_file = file_name if args['load_model'] == \"default\" else args['load_model']\n",
    "        policy.load(f\"./models/{policy_file}\")\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6253965b-d630-41ca-9543-d3764d6188ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = load_policy(\"MyAntBulletEnv-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8037f-f28a-4c14-b9fa-f46ff0639e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MyAntBulletEnv-v0\", render=True)\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690c03e-11bd-4c41-8693-fd6dd6ae1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, done = env.reset(), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0f18c-8b27-400f-8cf4-f8a89907fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759966cc-d37a-4585-9c09-f94b2a1dbfa3",
   "metadata": {},
   "source": [
    "Now that everything's set up, you can view the ant and **step through the simulation using the next cell** (we could have even just made that it's own function - call it 'advance' or something).  Control-enter will run the cell without advancing to the next cell.\n",
    "\n",
    "At this point, it's fun to keep the simulation window and the notebook both visible (I shrink my notebook to see the window on the side).  The view can be adjusted with control-click and your mouse wheel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329aa614-2870-46c1-a88b-efcc9a3f7c1f",
   "metadata": {},
   "source": [
    "**This cell can be used to advance the ant, one step at a time:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8109a-65da-4f32-ab12-7e6e1e2c7507",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = policy.select_action(np.array(state))\n",
    "state, reward, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738c38f7-0888-46c6-bbdd-27c4026afd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.robot.body_xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e2a58-37e7-479b-a64d-9073179f8d4b",
   "metadata": {},
   "source": [
    "Maybe mid-course we want to change the target the ant is walking to (which is then in the obs space):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7bbeba-85f8-4c8e-be27-e6cb262a51fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.robot.walk_target_x = -10\n",
    "env.robot.walk_target_y = -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6ff85-bbe2-404d-a431-16791861b6c0",
   "metadata": {},
   "source": [
    "Go back to the cell above and advance the ant. It should turn and go toward this new target.  There might be some momentum built up and some maneuvering to turn around, so advance it enough times to actually see it turn and start walking again.\n",
    "\n",
    "You get the picture - the policy controls what actions are taken, it's an artifact that we save and load, but other than that it's up to us how it gets used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da606f-d032-41b2-b6e2-3f2ee6cb8d6a",
   "metadata": {},
   "source": [
    "## Walk to each coordinate\n",
    "For fun - lets set it up so we can pass it a list of points the ant needs to go to, and we can pass in walking-path coordinates.  Maybe this list could be provided by another path-finding AI, or classical control scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f195d6-325e-409d-92c1-ee8f8f2114c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [(3,3),(0,3),(-6,-6),(0,-6),(9,9),(0,9),(0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbdb515-dbc1-4bcb-a38b-e5bf2f71ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in my_list:\n",
    "    env.robot.walk_target_x = i[0]\n",
    "    env.robot.walk_target_y = i[1]\n",
    "    path_done = False\n",
    "    counter_i = 0\n",
    "    while not path_done:\n",
    "        action = policy.select_action(np.array(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        time.sleep(1. / 100.) #comment out to run at max local system speed\n",
    "        counter_i += 1\n",
    "        if counter_i > 500:\n",
    "            path_done = True\n",
    "            \n",
    "        dist = np.linalg.norm([i[0]-env.robot.body_xyz[0],i[1]-env.robot.body_xyz[1]])\n",
    "        if dist < 0.2:\n",
    "            path_done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd0e5d5-03fc-4b53-b403-f2432e8ccf7a",
   "metadata": {},
   "source": [
    "Don't let all that power go to your head.. poor little ant.\n",
    "\n",
    "Try changing the list up a few times and see the ant run different routes. \n",
    "\n",
    ">**Can you make a path that Ant will follow?**\n",
    "\n",
    "This particular policy is from the 5 million time-step custom ant with no reward modification - it will mostly get the job done, but there will be a few instances where it just doesn't make the next point happen (this is why we give it the 500 time-step counter time-out per episode).  \n",
    "\n",
    ">**For fun, try taking one of your modified ants with custom reward and send it through a similar challenge.**\n",
    "\n",
    "- What happens if you (by-hand) manually tweak some of the robot internal state values as it's moving?  \n",
    "- Is the ant robust to observation signal noise?  How might you modify the training course so it would handle real-world sensor noise/errors/corruptions that it might encounter (as if the policy were placed into a real robot)?\n",
    "- What modifications might you want to make to the environment that the ant is trained in? \n",
    "- The observables it sees - would they be general enough to handle the 'real world'?\n",
    "\n",
    "There are lots of things to consider and weigh when building out your RL environment, training, and how you use the policy, but hopefully by this point you can start to answer some of these questions and think about what you might do, yourself.  Best of luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1783fb-cd20-4685-88ac-8e3a60d97ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
