{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea7a976-08f4-4cb9-a94a-4166b7553765",
   "metadata": {},
   "source": [
    "## Teaching Ourselves To Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea813e-2463-4ecc-8ee4-0ba5f84f1811",
   "metadata": {},
   "source": [
    "Should you wish to follow along with our discussion, you can clone our provided repo [here](https://github.com/dillonroach/TD3): assuming that you have conda installed already, the first thing you'll want to do is to create a conda environment from the included environment.yml\n",
    "\n",
    "`conda env create -f environment.yml`\n",
    "\n",
    "After this, activate the environment, and then the first thing you'll want to do is launch jupyter lab and check out [TD3notebook.ipynb](https://github.com/dillonroach/TD3/blob/master/TD3notebook.ipynb) - this is a direct translation over from the author-provided `main.py`: all we've done is stashed the configuration variables into a dictionary, named `args`, and shoved all the code that would be executed normally into a function called `main()` so it can be called simply in the notebook.\n",
    "\n",
    "Looking more closely at `main()` now, you should see that the function is mostly concerned with setting values for variables to be used once you get to the `for` loop, which is where the meat of the work happens. In the following section, note that for the first `start_timesteps` number of time steps the action is simply filled from random sampling of possible choices; this helps fill the replay buffer and give a baseline before actual policy choices are made\n",
    "\n",
    "```     \n",
    "        if t < args['start_timesteps']:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = (\n",
    "                policy.select_action(np.array(state))\n",
    "                + np.random.normal(0, max_action * args['expl_noise'], size=action_dim)\n",
    "            ).clip(-max_action, max_action)\n",
    "```\n",
    "\n",
    "The bulk of the actual training happens in only a few lines; the below section takes the selected action from above, applies it to the environment and returns the new environment state, with reward and a done flag.\n",
    "\n",
    "```\n",
    "        next_state, reward, done, _ = env.step(action) \n",
    "        done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "        # Store data in replay buffer\n",
    "        replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Train agent after collecting sufficient data\n",
    "        if t >= args['start_timesteps']:\n",
    "            policy.train(replay_buffer, args['batch_size'])\n",
    "```\n",
    "\n",
    "Once the environment reaches the described `done` state, an environment reset is called and it starts the process all over again.  Every `eval_freq` number of time steps the machinery evaluates the policy against a number of episodes outside the training process, and saves the current policy for good measure.  That's it.  Nice having all the complicated heavy lifting already coded for us.\n",
    "\n",
    "If you run the notebook, as is, it will train for two million time steps with all the standard hyperparameters the authors set up and out will pop a policy that  allows the robot ant to sprint like the animation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab8edb5-10c1-47ae-a12e-142216e1e480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
