{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb21746f-0aee-40b7-89b3-43022751122f",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- Learning goals: Use lower level items as goals.  I.e. the method used to solve the problem.  Structure with subproblems and one story - subproblems become learning outcomes\n",
    "- Consider places where learners write some code (but always give them the code in md so they can copy/paste if needed)\n",
    "- see other todos inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec9084c-bb67-4703-bc93-a22894fcd88b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Day 1, Part B: Faster Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d498730-9fd7-4d71-aa0b-e628f266e3f2",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "- \n",
    "\n",
    "**TODO: Add learning goals**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef64d3-bbfd-47ef-9f0e-51562db5ca3e",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "- **Simulation environment**: Notice that this is not the same as the python/conda environment.  The simulation environment is the simulated world where the reinforcement learning takes place.  It provides opportunities for an agent to learn and explore, and ideally provides challenges that aid in efficient learning.\n",
    "- **Agent (aka actor or policy)**: An entity in the simulation environment that performs actions.  The agent could be a person, a robot, a car, a thermostat, etc.\n",
    "- **State variable**: An observed variable in the simulation environment.  They can be coordinates of objects or entities, an amount of fuel in a tank, air temperature, wind speed, etc.\n",
    "- **Action variable**: An action that the agent can perform.  Examples: step forward, increase velocity to 552.5 knots, push object left with force of 212.3 N, etc.\n",
    "- **Reward**: A value given to the agent for doing something considered to be 'good'.  Reward is commonly assigned at each time step and cumulated during a learning episode.\n",
    "- **Episode**: A learning event consisting of multiple steps in which the agent can explore.  It starts with the unmodified environment and continues until the goal is achieved or something prevents further progress, such as a robot getting stuck in a hole.  Multiple episodes are typically run in loops until the model is fully trained.\n",
    "- **Model (aka policy or agent)**: An RL model is composed of the modeling architecture (e.g., neural network) and parameters or weights that define the unique behavior of the model.\n",
    "- **Policy (aka model or agent)**: The parameters of a model that encode the best choices to make in an environment.  The choices are not necessarily good ones until the model undergoes training.  The policy (or model) is the \"brain\" of the agent.\n",
    "- **Replay Buffer**: A place in memory to store state, action, reward and other variables describing environmental state transitions. It is effectively the agent's memory of past experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1526bd5-ae3e-4bb3-af45-036178cc92a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Reinforcement Learning Cycle](./images/Reinforcement-learning-diagram-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f59b66-9588-4e5c-8b65-92d03739556c",
   "metadata": {},
   "source": [
    "## Options for increasing learning rate\n",
    "- Multiprocessing (multiple instances of cartpole).  Also give examples of algorithms designed for shared learning (MATD3?)\n",
    "- Better reward functions (discussed in other notebooks)\n",
    "- Other tricks (e.g., move agent farther from target as learning progresses)\n",
    "- Add attention or other innovative algorithm enhancements\n",
    "\n",
    "**TODO: write out the bullets above**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d60ca2-4905-466c-b5dd-d71113c70344",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multiprocessing and Shared Learning\n",
    "\n",
    "Because RL is focused on decision sequences in which an agent's actions influence future environment states, it is difficult to parallelize computations and speed up learning.  Multi-agent techniques, however, allow shared learning from identical agents.  OpenAI Gym provides a method for this by creating multiple identical environments, each with an identical agent.  By giving each environment a different random seed, they each begin learning in a different state.  These environments can run on seperate processes on a single computer at the same time, and the experiences of all of the agents will be shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b63e8-0192-41a9-9c85-736f1732667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348acc61-503b-4b56-a3b3-ddd8d3c5df87",
   "metadata": {},
   "source": [
    "This function helps us make multiple environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b4a95-2a2e-4886-be46-07b7c4ad1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, rank, seed=0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environments you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = gym.make(env_id)\n",
    "        env.seed(seed + rank)\n",
    "        return env\n",
    "    \n",
    "    set_random_seed(seed)\n",
    "    \n",
    "    return _init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820229a1-78b2-4301-8704-39d959133008",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2720f89-0544-4a2c-b6ef-53144d7c5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556d0f3-53cf-44b1-85c6-b2d9d4d83b54",
   "metadata": {},
   "source": [
    "Let's use multiple cpus, but avoid using all of them so that the computer remains stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab5ed1-b9a8-45eb-a59b-334581c0985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpu = os.cpu_count()\n",
    "if num_cpu > 2:\n",
    "    num_cpu -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eac7ab-d85e-44f4-9179-9603d086ed96",
   "metadata": {},
   "source": [
    "Create the vectorized environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99efffc-c911-4637-a83e-8eca2a05b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefd287-ee53-44ad-8d91-e0f6a2d0e0df",
   "metadata": {},
   "source": [
    "Start learning then display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0712968-084a-4232-bf94-324c46cb0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbaacf8-c098-41b4-8867-d6191947184a",
   "metadata": {},
   "source": [
    "This will show the results from all of the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e77f1-a1b8-4741-918d-2574f07e9a5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0760671b-64fa-40ad-9201-498b3d30d4a2",
   "metadata": {},
   "source": [
    "\n",
    "**TODO: Show training loss plot and reward plot**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da16c5-a16e-4182-9b15-d6927fda8403",
   "metadata": {},
   "source": [
    "You can close the windows by restarting the kernel\n",
    "\n",
    "**TODO: Figure out how to close the env windows without restarting the kernel**\n",
    "\n",
    "**Maybe don't include parking environment**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
