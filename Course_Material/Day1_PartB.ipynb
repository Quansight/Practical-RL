{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec9084c-bb67-4703-bc93-a22894fcd88b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Day 1, Part B: More on Reward Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0490661-ce40-4fdb-9b13-9d4d99187442",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "- Further examine the effects of reward function changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd1444-09b3-4413-92df-aa19c3a2fcb3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Definitions\n",
    "- **Simulation environment**: Notice that this is not the same as the python/conda environment.  The simulation environment is the simulated world where the reinforcement learning takes place.  It provides opportunities for an agent to learn and explore, and ideally provides challenges that aid in efficient learning.\n",
    "- **Agent (aka actor or policy)**: An entity in the simulation environment that performs actions.  The agent could be a person, a robot, a car, a thermostat, etc.\n",
    "- **State variable**: An observed variable in the simulation environment.  They can be coordinates of objects or entities, an amount of fuel in a tank, air temperature, wind speed, etc.\n",
    "- **Action variable**: An action that the agent can perform.  Examples: step forward, increase velocity to 552.5 knots, push object left with force of 212.3 N, etc.\n",
    "- **Reward**: A value given to the agent for doing something considered to be 'good'.  Reward is commonly assigned at each time step and cumulated during a learning episode.\n",
    "- **Episode**: A learning event consisting of multiple steps in which the agent can explore.  It starts with the unmodified environment and continues until the goal is achieved or something prevents further progress, such as a robot getting stuck in a hole.  Multiple episodes are typically run in loops until the model is fully trained.\n",
    "- **Model (aka policy or agent)**: An RL model is composed of the modeling architecture (e.g., neural network) and parameters or weights that define the unique behavior of the model.\n",
    "- **Policy (aka model or agent)**: The parameters of a model that encode the best choices to make in an environment.  The choices are not necessarily good ones until the model undergoes training.  The policy (or model) is the \"brain\" of the agent.\n",
    "- **Replay Buffer**: A place in memory to store state, action, reward and other variables describing environmental state transitions. It is effectively the agent's memory of past experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1526bd5-ae3e-4bb3-af45-036178cc92a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Reinforcement Learning Cycle](./images/Reinforcement-learning-diagram-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9c113-a1e0-45e7-b747-40aab2f5f7e9",
   "metadata": {},
   "source": [
    "## Modify the CartPole Reward\n",
    "\n",
    "We will work on reward modifications later in this course.  For now, you can try modifying the reward here/  We've included `MyCartPole.py` in the Course_Material folder - a subclassed step function from the main environment definition.  Because of the register step below, you'll need to restart the kernel every time you modify the reward, so we've included the imports cell below for easy access.\n",
    "\n",
    "To try different reward functions with the code below, your workflow should look like the following:\n",
    "\n",
    "1. Modify the reward section (below line 40 in the .py), *remember to save.*\n",
    "2. Restart your kernel and clear all outputs (probably don't want to rerun all the training above).\n",
    "3. Run the following 5 cells below to retrain and look at reward.\n",
    "4. Feel free to play around with the total_timesteps=25000 if you want a shorter/longer test of your new reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd01e8d-a9f7-4c57-b932-6ef8a0b75613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from tqdm import trange\n",
    "import hvplot.pandas  # This adds HoloViews plotting capability directly from a Pandas dataframe\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d072395-5f94-4008-ad80-dca3a5ea0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import registry, make, spec\n",
    "\n",
    "def register(id, *args, **kwargs):\n",
    "    if id in registry.env_specs:\n",
    "        return\n",
    "    else:\n",
    "        return gym.envs.registration.register(id, *args, **kwargs)\n",
    "\n",
    "register(id='MyCartPole-v1',\n",
    "         entry_point='MyCartPole:MyCartPoleEnv',\n",
    "         max_episode_steps=1000,\n",
    "         reward_threshold=2500.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457b0a4-6de7-411c-af34-7d2d51bae93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = gym.make(\"MyCartPole-v1\")\n",
    "env = Monitor(env, log_dir)\n",
    "model = PPO('MlpPolicy', env, verbose=0)\n",
    "model.learn(total_timesteps=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788199ab-be95-4e62-bd7b-e50920f1bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_reward = pd.DataFrame(pd.to_numeric(pd.read_csv(\"tmp/monitor.csv\")[1:].reset_index()['index'])).reset_index()\n",
    "training_reward.rename(columns={'level_0':\"Episode\",'index':\"Reward\"},inplace=True)\n",
    "training_reward.hvplot(x=\"Episode\",y=\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1cd98-5359-497d-8b9a-7183049dac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "episode_reward = 0\n",
    "obs = env.reset()\n",
    "for _ in trange(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward \n",
    "    env.render()\n",
    "    if done:\n",
    "        reward_list.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        env.reset()\n",
    "env.env.viewer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128f52d-1040-4eac-9498-3e82e2ce45ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2d7ba5-a349-4da0-a7e6-0ca071797783",
   "metadata": {},
   "source": [
    ">**Did your change in the reward function have the outcome you expected?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
