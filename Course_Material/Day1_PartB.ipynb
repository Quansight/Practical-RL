{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec9084c-bb67-4703-bc93-a22894fcd88b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Day 1, Part B: Faster Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d498730-9fd7-4d71-aa0b-e628f266e3f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Learning goals\n",
    "- Learning can be accelerated by a number of factors\n",
    "- Test multi-environment training using Subprocess Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef64d3-bbfd-47ef-9f0e-51562db5ca3e",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "- **Simulation environment**: Notice that this is not the same as the python/conda environment.  The simulation environment is the simulated world where the reinforcement learning takes place.  It provides opportunities for an agent to learn and explore, and ideally provides challenges that aid in efficient learning.\n",
    "- **Agent (aka actor or policy)**: An entity in the simulation environment that performs actions.  The agent could be a person, a robot, a car, a thermostat, etc.\n",
    "- **State variable**: An observed variable in the simulation environment.  They can be coordinates of objects or entities, an amount of fuel in a tank, air temperature, wind speed, etc.\n",
    "- **Action variable**: An action that the agent can perform.  Examples: step forward, increase velocity to 552.5 knots, push object left with force of 212.3 N, etc.\n",
    "- **Reward**: A value given to the agent for doing something considered to be 'good'.  Reward is commonly assigned at each time step and cumulated during a learning episode.\n",
    "- **Episode**: A learning event consisting of multiple steps in which the agent can explore.  It starts with the unmodified environment and continues until the goal is achieved or something prevents further progress, such as a robot getting stuck in a hole.  Multiple episodes are typically run in loops until the model is fully trained.\n",
    "- **Model (aka policy or agent)**: An RL model is composed of the modeling architecture (e.g., neural network) and parameters or weights that define the unique behavior of the model.\n",
    "- **Policy (aka model or agent)**: The parameters of a model that encode the best choices to make in an environment.  The choices are not necessarily good ones until the model undergoes training.  The policy (or model) is the \"brain\" of the agent.\n",
    "- **Replay Buffer**: A place in memory to store state, action, reward and other variables describing environmental state transitions. It is effectively the agent's memory of past experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1526bd5-ae3e-4bb3-af45-036178cc92a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Reinforcement Learning Cycle](./images/Reinforcement-learning-diagram-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f59b66-9588-4e5c-8b65-92d03739556c",
   "metadata": {},
   "source": [
    "## Some options for increasing learning rate\n",
    "- Multiprocessing (multiple instances of cartpole) - we go into this topic below where we create multiple environments and share learning across them.\n",
    "- Better reward functions, as we briefly touched on in earlier parts, and will return to in Day 4\n",
    "- Tricks like moving the agent farther from target as learning progresses or gradual task-adjustment environment changes\n",
    "- Add attention or other innovative algorithm enhancements from new research articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d60ca2-4905-466c-b5dd-d71113c70344",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multiprocessing and Shared Learning\n",
    "\n",
    "Because RL is focused on decision sequences in which an agent's actions influence future environment states, it is difficult to parallelize computations and speed up learning.  Multi-agent techniques, however, allow shared learning from identical agents.  OpenAI Gym provides a method for this by creating multiple identical environments, each with an identical agent.  By giving each environment a different random seed, they each begin learning in a different state.  These environments can run on seperate processes on a single computer at the same time, and the experiences of all of the agents will be shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b63e8-0192-41a9-9c85-736f1732667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348acc61-503b-4b56-a3b3-ddd8d3c5df87",
   "metadata": {},
   "source": [
    "This function helps us make multiple environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b4a95-2a2e-4886-be46-07b7c4ad1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, rank, seed=0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environments you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = gym.make(env_id)\n",
    "        env.seed(seed + rank)\n",
    "        return env\n",
    "    \n",
    "    set_random_seed(seed)\n",
    "    \n",
    "    return _init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820229a1-78b2-4301-8704-39d959133008",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2720f89-0544-4a2c-b6ef-53144d7c5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556d0f3-53cf-44b1-85c6-b2d9d4d83b54",
   "metadata": {},
   "source": [
    "Let's use multiple cpus, but avoid using all of them so that the computer remains stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab5ed1-b9a8-45eb-a59b-334581c0985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpu = os.cpu_count()\n",
    "if num_cpu > 2:\n",
    "    num_cpu -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eac7ab-d85e-44f4-9179-9603d086ed96",
   "metadata": {},
   "source": [
    "Create the vectorized environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99efffc-c911-4637-a83e-8eca2a05b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefd287-ee53-44ad-8d91-e0f6a2d0e0df",
   "metadata": {},
   "source": [
    "Start learning then display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214e45f9-ee4d-498f-89a0-6769987c29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0712968-084a-4232-bf94-324c46cb0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_dir, n_steps=256)\n",
    "model.learn(total_timesteps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6e473-868a-4ed1-bbb3-6b1f9f943c9e",
   "metadata": {},
   "source": [
    "Now try modifying the n_steps and total_timesteps above to see how it affects training time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbaacf8-c098-41b4-8867-d6191947184a",
   "metadata": {},
   "source": [
    "We now save the model and reload from a single cartpole env to view the results of our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99df45-5e89-48b0-a84a-917e198dbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_cartpole\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003428d-9b42-4c8e-825d-e4075af6e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "model = PPO.load(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e77f1-a1b8-4741-918d-2574f07e9a5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "env.env.viewer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0760671b-64fa-40ad-9201-498b3d30d4a2",
   "metadata": {},
   "source": [
    "Run cell below to show training loss plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb0934-6df0-4252-b4d2-bc9e0c5a2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir ./tmp/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
