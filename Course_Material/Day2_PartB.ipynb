{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea7a976-08f4-4cb9-a94a-4166b7553765",
   "metadata": {},
   "source": [
    "# Day 2, Part B: TD3 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c109ed4-29f7-4dee-9201-c7dcda12151c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning goals\n",
    "- Find out why TD3 is more performant for this environment than PPO\n",
    "- Walk through the TD3 code and learn how this author constructed it\n",
    "- See examples of terminology useage that are different from the CartPole example\n",
    "- Learn what a replay buffer is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a900b8d3-cc30-4fb4-98ab-19a080b8f6a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Definitions\n",
    "- **Simulation environment**: Notice that this is not the same as the python/conda environment.  The simulation environment is the simulated world where the reinforcement learning takes place.  It provides opportunities for an agent to learn and explore, and ideally provides challenges that aid in efficient learning.\n",
    "- **Agent (aka actor or policy)**: An entity in the simulation environment that performs actions.  The agent could be a person, a robot, a car, a thermostat, etc.\n",
    "- **State variable**: An observed variable in the simulation environment.  They can be coordinates of objects or entities, an amount of fuel in a tank, air temperature, wind speed, etc.\n",
    "- **Action variable**: An action that the agent can perform.  Examples: step forward, increase velocity to 552.5 knots, push object left with force of 212.3 N, etc.\n",
    "- **Reward**: A value given to the agent for doing something considered to be 'good'.  Reward is commonly assigned at each time step and cumulated during a learning episode.\n",
    "- **Episode**: A learning event consisting of multiple steps in which the agent can explore.  It starts with the unmodified environment and continues until the goal is achieved or something prevents further progress, such as a robot getting stuck in a hole.  Multiple episodes are typically run in loops until the model is fully trained.\n",
    "- **Model (aka policy or agent)**: An RL model is composed of the modeling architecture (e.g., neural network) and parameters or weights that define the unique behavior of the model.\n",
    "- **Policy (aka model or agent)**: The parameters of a model that encode the best choices to make in an environment.  The choices are not necessarily good ones until the model undergoes training.  The policy (or model) is the \"brain\" of the agent.\n",
    "- **Replay Buffer**: A place in memory to store state, action, reward and other variables describing environmental state transitions. It is effectively the agent's memory of past experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be34d8cc-1062-43bc-8b01-b7ebfce3d585",
   "metadata": {},
   "source": [
    "## TD3 vs PPO\n",
    "\n",
    "One of the big differences between these two is that PPO is an on-policy method, while TD3 is an off-policy method.  On-policy means that the value of the next action is determined using the current actor policy, and off-policy means that the value is determined by a different function, such as a value function.\n",
    "\n",
    "In this specific case, TD3 builds two Q-functions (twin quality value functions) that map future expected rewards given the current action (current time step).  On the other hand, PPO makes all reward estimates by applying the current actor policy along multi-step trajectories.  By using the same policy to estimate rewards as the actor policy,  PPO needs to learn over more time steps to gain the same range of exploration as TD3.\n",
    "\n",
    "TD3 also builds a replay buffer as it learns off-policy.  This makes it more sample-efficient and therefore a great choice when simulations (or real-world robots) are slower than the algorithm.\n",
    "\n",
    "Off-policy methods tend to be less stable than on-policy methods, but TD3 has some tricks for reducing instability, which will be discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad67bb-d838-4670-a278-e9843b43258b",
   "metadata": {},
   "source": [
    "Check out [TD3notebook.ipynb](https://github.com/Quansight/Practical-RL/blob/main/TD3notebook.ipynb) - this is a direct translation from the author-provided `main.py`: all we've done is stashed the configuration variables into a dictionary, named `args`, and shoved all the code that would be executed normally into a function called `main()` so it can be called simply in the notebook.\n",
    "\n",
    "In this notebook, let's walk through the code a bit.  To keep the notebook functional, we've removed the `main()` definition.  In general, the function is mostly concerned with setting values for variables to be used in the `for` loop, which is where the meat of the learning happens.  Let's look more closely..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b63ec-b287-4124-9f57-6acf071abab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6598607e-bd66-4867-a3ff-07252a0a7c5c",
   "metadata": {},
   "source": [
    "We have the original TD3 algorithm as a python file in this repo, so we can import it as a submodule and use it in the algorithm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ea07b9-fd75-46ee-8f5b-67babd36ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(Path().resolve().parent))\n",
    "import utils\n",
    "import TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f836a35-dcfd-4526-8976-2e973d1ec3c6",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "This [first function](https://github.com/sfujim/TD3/blob/master/main.py#L15) is used to evaluate the policy, either while the agent is learning or afterward when the model is fully trained.\n",
    "- It first makes a new environment with a fixed random seed\n",
    "- Then it loops through several learning episodes and records the reward earned from each one\n",
    "- The average reward is calculated, printed to the screen, and returned to the calling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf10c2-9637-476c-b071-046c6d70e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs policy for X episodes and returns average reward\n",
    "# A fixed seed is used for the eval environment\n",
    "def eval_policy(policy, env_name, seed, eval_episodes=10):\n",
    "    eval_env = gym.make(env_name)\n",
    "    eval_env.seed(seed + 100)\n",
    "\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    \n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed8c239-97b5-4f33-ab66-d1e937b4b7a2",
   "metadata": {},
   "source": [
    "## Variables and Initialization\n",
    "\n",
    "This first part of the code is simply a dictionary of parameters to be specified for the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae688383-8303-494b-8044-c6c9382437e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "        \"policy\" : \"TD3\",                  # Policy name\n",
    "        \"env\" : \"AntBulletEnv-v0\",         # OpenAI gym environment name\n",
    "        \"seed\" : 0,                        # Sets Gym, PyTorch and Numpy seeds\n",
    "        \"start_timesteps\" : 25e3,          # Time steps initial random policy is used\n",
    "        \"eval_freq\" : 5e3,                 # How often (time steps) we evaluate\n",
    "        \"max_timesteps\" : 2e6,             # Max time steps to run environment\n",
    "        \"expl_noise\" : 0.1,                # Std of Gaussian exploration noise\n",
    "        \"batch_size\" : 256,                # Batch size for both actor and critic\n",
    "        \"discount\" : 0.99,                 # Discount factor\n",
    "        \"tau\" : 0.005,                     # Target network update rate\n",
    "        \"policy_noise\" : 0.2,              # Noise added to target policy during critic update\n",
    "        \"noise_clip\" : 0.5,                # Range to clip target policy noise\n",
    "        \"policy_freq\" : 2,                 # Frequency of delayed policy updates\n",
    "        \"save_model\" : \"store_true\",       # Save model and optimizer parameters\n",
    "        \"load_model\" : \"\",                 # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "       }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152f2a3-0675-4c88-a0b3-0317c8aaa70c",
   "metadata": {},
   "source": [
    "Make a file name to keep track of the models we've made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca222d-a1ab-4225-b6cc-1d32569209a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f\"{args['policy']}_{args['env']}_{args['seed']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be173a4f-28af-45a4-9303-753f6bdab851",
   "metadata": {},
   "source": [
    "Make sure some subfolders are present to save the results and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ceb109-5a48-461c-949b-e3f990e7ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./results\"):\n",
    "    os.makedirs(\"./results\")\n",
    "\n",
    "if args['save_model'] and not os.path.exists(\"./models\"):\n",
    "    os.makedirs(\"./models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d208b1-d953-4406-b0f8-a04b5cbebf08",
   "metadata": {},
   "source": [
    ">**In the next cell, make the gym environment just like we did in the CartPole example.**  Use `args['env']` as the environment name and return the usual `env` object.\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "env = gym.make(args['env'])\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d6a1f-c9f0-400b-8645-d94e8b1e8cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46c4fdd4-f7e8-4473-bab8-127511ab3e71",
   "metadata": {},
   "source": [
    "Set the random seeds for the environment, Torch (if we run on GPU), and NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc4efa2-ebc8-4605-a325-2b2c6d40bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(args['seed'])\n",
    "env.action_space.seed(args['seed'])\n",
    "torch.manual_seed(args['seed'])\n",
    "np.random.seed(args['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a38a7d-2885-40de-befa-a54138357dc2",
   "metadata": {},
   "source": [
    "We need the algorithm (TD3) to know some things about the environment, including the dimensions of the state and action spaces.  TD3 also needs to know the largest action value to expect.\n",
    "\n",
    ">Try printing some of the following values to get a better understanding of what values are being passed to TD3 (**just print the kwargs dict**).  The state dimensions might be larger than you expected.  If you go to the walker base class for pybullet there is a `calc_state` function ([here](https://github.com/bulletphysics/bullet3/blob/a62fb187a5c83a2e1e3e0376565ab3ae47870465/examples/pybullet/gym/pybullet_envs/robot_locomotors.py#L35)).  See if you can find a few of the state variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d6840-d52f-41e9-baa6-c3b513587c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0] \n",
    "max_action = float(env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec33f175-7afd-4a0b-b5b4-3ab4425c07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"state_dim\": state_dim,\n",
    "    \"action_dim\": action_dim,\n",
    "    \"max_action\": max_action,\n",
    "    \"discount\": args['discount'],\n",
    "    \"tau\": args['tau'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ef519-c46a-4f6e-8489-795bae86a6e0",
   "metadata": {},
   "source": [
    "## TD3 Tricks\n",
    "\n",
    "TD3 is an improvement upon DDPG.  Some folks refer to those improvements as \"tricks\" because they are fairly simple.\n",
    "\n",
    "One way to improve exploration is to simply add noise to the actions during learning.  This ensures that the decisions made by the agent are not the same every time.  Even as the agent learns better actions, it will continue to try actions that are at least a little bit different from the known high-reward actions.\n",
    "\n",
    "As you read on OpenAI Spinning Up, they list the three \"tricks\":\n",
    ">**Trick One: Clipped Double-Q Learning**. TD3 learns two Q-functions instead of one (hence “twin”), and uses the smaller of the two Q-values to form the targets in the Bellman error loss functions.\n",
    ">\n",
    ">**Trick Two: “Delayed” Policy Updates**. TD3 updates the policy (and target networks) less frequently than the Q-function. The paper recommends one policy update for every two Q-function updates.\n",
    ">\n",
    ">**Trick Three: Target Policy Smoothing**. TD3 adds noise to the target action, to make it harder for the policy to exploit Q-function errors by smoothing out Q along changes in action.\n",
    "\n",
    "The three variables below are each used in the tricks, and the noise variables are scaled to the action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102735be-8abf-4d04-a3bd-3c88319e9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trick One\n",
    "kwargs[\"noise_clip\"] = args['noise_clip'] * max_action\n",
    "# Trick Two\n",
    "kwargs[\"policy_freq\"] = args['policy_freq']\n",
    "# Trick Three\n",
    "kwargs[\"policy_noise\"] = args['policy_noise'] * max_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66cfa83-eaff-4c58-8e98-4b884d7dcfd0",
   "metadata": {},
   "source": [
    ">**In your own words, write a description of each of the tricks, stating cleary why they help learning.**  Feel free to review the Spinning up descriptions and reviewing the TD3 paper.  Some explanation is given in this notebook too.  We will ask three of you to describe one of the tricks.  As we discuss them, feel free to update your description.\n",
    "\n",
    "- Trick One: (type answer here)\n",
    "- Trick Two: (type answer here)\n",
    "- Trick Three: (type answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb4805-9ed1-4779-ac5b-7f9e007e0e84",
   "metadata": {},
   "source": [
    "Initialize the TD3 policy.\n",
    "\n",
    ">**But first, go back to the CartPole example (Day1, Part A) and find the cell where we created an instance of the PPO algorithm.  What name did we give PPO in that case?  What name does the author of TD3 give below?**\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "For CartPole, we followed OpenAI's convention of naming the algorithm \"model\", but here, TD3 is given the name \"policy\".  This kind of inconsistency in terminology is common in RL, so keep in mind that \"model\" and \"policy\" are equivalent between these two examples.  You might see \"agent\" or \"actor\" used in other code as well.\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabaf7c0-4bfc-47b9-a8d8-2b52f54ae55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = TD3.TD3(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac5e03-d26f-4489-a544-5fa0d99023e2",
   "metadata": {},
   "source": [
    "This cell just loads a previous model or starts a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f8f4c-35e7-4ce9-96a6-a18ba818e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['load_model'] != \"\":\n",
    "    policy_file = file_name if args['load_model'] == \"default\" else args['load_model']\n",
    "    policy.load(f\"./models/{policy_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8232b0a6-3ffe-4612-a185-efe2c180fc25",
   "metadata": {},
   "source": [
    "## Experience Replay Buffer\n",
    "\n",
    "This buffer is what keeps track of past experiences.  The algorithm will sample from this buffer to estimate the value of the agent's next action.  The buffer does not keep all experiences, but ideally it keeps a representative range of them.\n",
    "\n",
    "The experiences are state transitions tied to actions and rewards.  \n",
    "\n",
    ">**Look at the file `utils.py` for what else is stored in the buffer.  Describe the values that you can by listing them here.**\n",
    "\n",
    "- (type answer here)\n",
    "- (type answer here)\n",
    "- (type answer here)\n",
    "- (type answer here)\n",
    "- (type answer here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c6a37a-b8c7-477f-b05a-739bc23309de",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = utils.ReplayBuffer(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822821c8-de4d-4398-9502-b487c252a1c4",
   "metadata": {},
   "source": [
    "## Learning over many episodes\n",
    "\n",
    "Scan through the code in the next cell, then keep reading to learn about parts of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fcac26-f334-415c-a882-655d2c1fe8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate untrained policy and save as the first one in a sequence of trained policies\n",
    "evaluations = [eval_policy(policy, args['env'], args['seed'])]\n",
    "\n",
    "state, done = env.reset(), False\n",
    "episode_reward = 0\n",
    "episode_timesteps = 0\n",
    "episode_num = 0\n",
    "\n",
    "for t in range(int(args['max_timesteps'])):\n",
    "\n",
    "    episode_timesteps += 1\n",
    "\n",
    "    # Select action randomly or according to policy\n",
    "    if t < args['start_timesteps']:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = (\n",
    "            policy.select_action(np.array(state))\n",
    "            + np.random.normal(0, max_action * args['expl_noise'], size=action_dim)\n",
    "        ).clip(-max_action, max_action)\n",
    "\n",
    "    # Perform action\n",
    "    next_state, reward, done, _ = env.step(action) \n",
    "    done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "    # Store data in replay buffer\n",
    "    replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "\n",
    "    # Train agent after collecting sufficient data\n",
    "    if t >= args['start_timesteps']:\n",
    "        policy.train(replay_buffer, args['batch_size'])\n",
    "\n",
    "    if done: \n",
    "        # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "        \n",
    "        # Reset environment\n",
    "        state, done = env.reset(), False\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1 \n",
    "\n",
    "    # Evaluate episode\n",
    "    if (t + 1) % args['eval_freq'] == 0:\n",
    "        evaluations.append(eval_policy(policy, args['env'], args['seed']))\n",
    "        np.save(f\"./results/{file_name}\", evaluations)\n",
    "        if args['save_model']: \n",
    "            policy.save(f\"./models/{file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f53bdd-2394-4e0f-8fae-55c3c00a1311",
   "metadata": {},
   "source": [
    "In the following section, note that for the first `start_timesteps` number of time steps, the action is simply filled from random sampling of possible choices; this helps fill the replay buffer and give a baseline before actual policy choices are made.\n",
    "\n",
    "```python     \n",
    "    if t < args['start_timesteps']:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = (\n",
    "            policy.select_action(np.array(state))\n",
    "            + np.random.normal(0, max_action * args['expl_noise'], size=action_dim)\n",
    "        ).clip(-max_action, max_action)\n",
    "```\n",
    "\n",
    "The bulk of the actual training happens in only a few lines.  The below section takes the selected action from above, applies it to the environment, and returns the new environment state, including the reward and a done flag.  It then checks whether the number of time steps reached the maximum or not.\n",
    "\n",
    "```python\n",
    "    next_state, reward, done, _ = env.step(action) \n",
    "    done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "```\n",
    "\n",
    "The outcome of the time step is saved to the experience replay buffer.\n",
    "\n",
    "```python\n",
    "    replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "```\n",
    "Then the code updates the state, saves the reward, and, if the replay buffer has recieved enough baseline values, trains the policy.  At this point, the ant will explore the environment by trying to move its legs such that it receives high rewards.\n",
    "\n",
    "```python\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "\n",
    "    # Train agent after collecting sufficient data\n",
    "    if t >= args['start_timesteps']:\n",
    "        policy.train(replay_buffer, args['batch_size'])\n",
    "```\n",
    "\n",
    "Once the environment reaches the described `done` state, the environment and some variables are reset.\n",
    "\n",
    "```python\n",
    "    if done: \n",
    "        # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "        \n",
    "        # Reset environment\n",
    "        state, done = env.reset(), False\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1 \n",
    "```\n",
    "\n",
    "Before starting a new episode, every `eval_freq` number of time steps, the policy is evaluated against a number of episodes outside the training process, and saves the current policy for good measure.\n",
    "\n",
    "```python\n",
    "    # Evaluate episode\n",
    "    if (t + 1) % args['eval_freq'] == 0:\n",
    "        evaluations.append(eval_policy(policy, args['env'], args['seed']))\n",
    "        np.save(f\"./results/{file_name}\", evaluations)\n",
    "        if args['save_model']: \n",
    "            policy.save(f\"./models/{file_name}\")\n",
    "```\n",
    "\n",
    "That's it.  It's nice having all the complicated heavy lifting already coded for us.  \n",
    "\n",
    "If you run the notebook, as is, it will train for two million time steps with all the standard hyperparameters the TD3 authors set up and out will pop a policy that  allows the robot ant to sprint like the animation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab8edb5-10c1-47ae-a12e-142216e1e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Image(\"../animations/base_ant.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
