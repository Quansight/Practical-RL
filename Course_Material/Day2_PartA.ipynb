{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea7a976-08f4-4cb9-a94a-4166b7553765",
   "metadata": {},
   "source": [
    "# Day 2, Part A: Learning To Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c109ed4-29f7-4dee-9201-c7dcda12151c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning goals\n",
    "\n",
    "- goals here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a900b8d3-cc30-4fb4-98ab-19a080b8f6a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Definitions\n",
    "- **Simulation environment**: Notice that this is not the same as the python/conda environment.  The simulation environment is the simulated world where the reinforcement learning takes place.  It provides opportunities for an agent to learn and explore, and ideally provides challenges that aid in efficient learning.\n",
    "- **Agent (aka actor or policy)**: An entity in the simulation environment that performs actions.  The agent could be a person, a robot, a car, a thermostat, etc.\n",
    "- **State variable**: An observed variable in the simulation environment.  They can be coordinates of objects or entities, an amount of fuel in a tank, air temperature, wind speed, etc.\n",
    "- **Action variable**: An action that the agent can perform.  Examples: step forward, increase velocity to 552.5 knots, push object left with force of 212.3 N, etc.\n",
    "- **Reward**: A value given to the agent for doing something considered to be 'good'.  Reward is commonly assigned at each time step and cumulated during a learning episode.\n",
    "- **Episode**: A learning event consisting of multiple steps in which the agent can explore.  It starts with the unmodified environment and continues until the goal is achieved or something prevents further progress, such as a robot getting stuck in a hole.  Multiple episodes are typically run in loops until the model is fully trained.\n",
    "- **Model (aka policy or agent)**: An RL model is composed of the modeling architecture (e.g., neural network) and parameters or weights that define the unique behavior of the model.\n",
    "- **Policy (aka model or agent)**: The parameters of a model that encode the best choices to make in an environment.  The choices are not necessarily good ones until the model undergoes training.  The policy (or model) is the \"brain\" of the agent.\n",
    "- **Replay Buffer**: A place in memory to store state, action, reward and other variables describing environmental state transitions. It is effectively the agent's memory of past experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f7fbf-cb66-4ac7-9a5f-f13770dc4b75",
   "metadata": {},
   "source": [
    "## The Ant Environment\n",
    "\n",
    "We're going to use the Ant environment for the rest of the course.  This environment provides a fairly complex learning challenge and we can modify it to understand RL at a deeper and more useful level.  The default goal is for the ant (agent) to learn to move as fast as it can in one direction.  The animation below shows that the ant will eventually learn to run.\n",
    "\n",
    "The environment is a flat world with no topography or other objects, which will avoid complex situations and allow the ant to focus on learning to run.  Remember that the ant begins with no knowledge of how to move at all.  It has eight joints, four at the body and four midway down the legs.  The actions available to it are simply a set of motor torque values for the eight joints.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8706df1-f719-4e48-8a88-db6208d6549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Image(\"../animations/base_ant.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a473b-0b4a-4725-9bc8-feaba42e3b48",
   "metadata": {},
   "source": [
    "In the previous exercises, we used the the CartPole environment and the PPO algorithm.  The environment has two discrete action variables (+1, -1).  Ant uses eight continuous variables, each in the range 0-1.  Because we're now working in a continuous action space, we have other options for RL algorithms.  PPO works for both discrete and continous actions, but it tends to need more learning examples to converge on a good policy.  TD3 is a more efficient algorithm, and it's one of the best for continous action spaces, so we will work with it.\n",
    "\n",
    ">Why do we prefer more efficient algorithms for continous action spaces?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Continous variables can have a huge range of values.  On a common 64 bit computer, there can be around 2^62 values between 0 and 1, and that's just for one variable.  With effectively infinite state-action combinations, we need an algorithm that can find a policy as efficiently as possible.\n",
    "</details>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
