{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be02bf3-ae44-488e-802b-edf7cdd3b738",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- Learning goals: Use lower level items as goals.  I.e. the method used to solve the problem.  Structure with subproblems and one story - subproblems become learning outcomes\n",
    "- Consider places where learners write some code (but always give them the code in md so they can copy/paste if needed)\n",
    "- see other todos inline\n",
    "- update the 'about' section of the repo\n",
    "- update all definitions with the ones here (edits are being made in this notebook)\n",
    "- test on Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec9084c-bb67-4703-bc93-a22894fcd88b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Day 1, Part A: Introduction to reinforcement learning and research environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0490661-ce40-4fdb-9b13-9d4d99187442",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "In addition to understanding principles of reinforcement learning (RL) and the tools we use to experiment in RL, you will learn several other things:\n",
    "- Ability to use common terminology in RL\n",
    "- Understand the importance of each step of the RL cycle\n",
    "- How to install and run a model in an OpenAI Gym environment\n",
    "- Think carefully about reward functions, and discuss possible improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd1444-09b3-4413-92df-aa19c3a2fcb3",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "- **Simulation environment**: Notice that this is not the same as the python/conda environment.  The simulation environment is the simulated world where the reinforcement learning takes place.  It provides opportunities for an agent to learn and explore, and ideally provides challenges that aid in efficient learning.\n",
    "- **Agent (aka actor or policy)**: An entity in the simulation environment that performs actions.  The agent could be a person, a robot, a car, a thermostat, etc.\n",
    "- **State variable**: An observed variable in the simulation environment.  They can be coordinates of objects or entities, an amount of fuel in a tank, air temperature, wind speed, etc.\n",
    "- **Action variable**: An action that the agent can perform.  Examples: step forward, increase velocity to 552.5 knots, push object left with force of 212.3 N, etc.\n",
    "- **Reward**: A value given to the agent for doing something considered to be 'good'.  Reward is commonly assigned at each time step and cumulated during a learning episode.\n",
    "- **Episode**: A learning event consisting of multiple steps in which the agent can explore.  It starts with the unmodified environment and continues until the goal is achieved or something prevents further progress, such as a robot getting stuck in a hole.  Multiple episodes are typically run in loops until the model is fully trained.\n",
    "- **Model (aka policy or agent)**: An RL model is composed of the modeling architecture (e.g., neural network) and parameters or weights that define the unique behavior of the model.\n",
    "- **Policy (aka model or agent)**: The parameters of a model that encode the best choices to make in an environment.  The choices are not necessarily good ones until the model undergoes training.  The policy (or model) is the \"brain\" of the agent.\n",
    "- **Replay Buffer**: A place in memory to store state, action, reward and other variables describing environmental state transitions. It is effectively the agent's memory of past experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb42be-4861-4361-9bf1-a91ad2191e5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A Technical Overview of Reinforcement Learning (RL)\n",
    "\n",
    "For a less technical overview of RL, please read [the blog post](https://www.quansight.com/post/exploring-reinforcement-learning).\n",
    "\n",
    "Learning a new skill can sometimes be daunting - maybe you read books, take a class, watch videos of people who already know - maybe you prefer to simply dive in and make a mess of things as you go, learning from the process. Similarly, there are a number of ways in which a computer model can learn a new skill. In the world of machine learning, RL takes the latter approach of diving right in and trying things out to see how they go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1526bd5-ae3e-4bb3-af45-036178cc92a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RL Cycle\n",
    "\n",
    "RL is used to train a model to perform one or more tasks.  It is often applied when we do not have data that can be used to train a supervised learning model.  Instead, we set up a simulated environment and give an agent the ability to explore and learn about the environment.  The agent is a representation of something in the environment that can perform actions.  It could be a person, a vehicle, a light switch, a control circuit, etc.  In the diagram below, it is a robot.\n",
    "\n",
    "At first, the agent knows nothing about the environment, but it can make observations of the state variables in the environment, such as locations and quantities of things, temperature, time, and so on.  We can give the agent a reward for doing something we consider good, and it will continue to seek rewards. In general, the more it explores and experiments with actions in the environment, the better it learns.\n",
    "\n",
    "![Reinforcement Learning Cycle](./images/Reinforcement-learning-diagram-01.png)\n",
    "\n",
    "The cycle typically completes a loop for each time step in the simulation. \n",
    "1. The model (part of the agent in this diagram) outputs an action vector\n",
    "2. The agent performs the action in the environment\n",
    "3. The environment usually changes\n",
    "4. The state variables and a reward value is passed to the model\n",
    "5. The model takes the new state observations as input and outputs a new action vector for the next time step\n",
    "\n",
    "Early learning can be comical as the agent stumbles around trying to learn, but as learning proceeds, the agent begins to develop better skills, and if given enough opportunity to learn, it can find interesting and innovative solutions.\n",
    "\n",
    "Below, see the basic ant trained to only 50k steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08906de-426b-4a5d-8a43-3b77fae45ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Image(\"images/base_ant_at50k.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc85c9f-dfc7-4e1c-ab4e-7b0947d5eb07",
   "metadata": {},
   "source": [
    "And at 2M training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee803a-1229-400b-8c4b-6ef20363eac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Image(\"../animations/base_ant.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35575a0a-05e0-4fcd-ae30-2727aa568d0f",
   "metadata": {},
   "source": [
    "The RL cycle diagram above is relatively simple.  At the core of the agent is a model, essentially the brain of the agent.  The agent also provides a way to interact with the environment, so we often associate observations and actions with it.  The diagram below attempts to define the agent more explicitly.  Notice that the policy is actually the set of parameters (weights) of the network.  The network inputs are the state variables, and the outputs are the action variables.\n",
    "\n",
    "![Agent Components](./images/rl_agent_huang_et_al_2019.png)\n",
    "\n",
    "[Huang et al., 2019](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8742652)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbe0c3c-b0ef-4190-a8ab-16ed859d8ec9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simulation Environments\n",
    "\n",
    "In general, a simulated environment should be as close to the real environment as possible without making the simulation overly complex.  This requires a balance between simulation details and computational efficiency, which is specific to each problem.  Each problem has different challenges that the agent must overcome and the environement will need to be a good place for the agent to learn.\n",
    "\n",
    "Most research and teaching in RL is done with simulated environments.  They also provide common ground for benchmarking a new algorithm and showing how well it performs relative to existing algorithms.  Several libraries of environments are available to do RL research, such as [OpenAI Gym](https://gym.openai.com/envs/) and [MuJoCo](https://deepmind.com/blog/announcements/mujoco).  They commonly require a Physics engine, which is a way to represent Physics in a computer simulation.  These engines enable things like gravity, friction, and so on. \n",
    "\n",
    "For this course, we will use OpenAI Gym environments paired with the [PyBullet3](https://github.com/bulletphysics/bullet3) Physics engine where needed (some environments are self-contained).  We chose these because they are free, easy to set up, and we can modify the reward function, something that is important in RL work.\n",
    "\n",
    "Let's look at one of these environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b922e324-ce6f-49bc-832a-ad93786dd1f5",
   "metadata": {},
   "source": [
    "#### Environment example: CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e09ac-707c-4131-a434-189914f71000",
   "metadata": {},
   "source": [
    "The [CartPole environment](https://gym.openai.com/envs/CartPole-v1/) runs quickly and provides a simple case for discussion.  It also does not need an external physics engine.  This description is from the Gym website:\n",
    "\n",
    ">A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over.\n",
    "\n",
    "Here is a gif showing a trained CartPole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb1ca5-ca32-4c46-a9db-79c9267a09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Image(\"images/cart_pole.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5196d3b-f91c-4b4a-8d4d-9d148cf52395",
   "metadata": {},
   "source": [
    "To make our own model, we'll need to import `gym` and several classes and methods from the `stable_baselines` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b63e8-0192-41a9-9c85-736f1732667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from tqdm import trange\n",
    "import hvplot.pandas  # This adds HoloViews plotting capability directly from a Pandas dataframe\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a076fe8f-4bcd-4eb3-be17-9dfb05403899",
   "metadata": {},
   "source": [
    "Gym makes it easy to make an environment.  First we'll set up logging so that we can more easily track progress of learning, and then we'll create the environment.  Notice that we are wrapping the environment with the `Monitor` class to track the data that is generated during learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ec9a9-cc22-47bc-83a6-4d5ff5a57887",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7edae4-ad40-42ac-94d5-94c545bb3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env = Monitor(env, log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9782853a-a82c-4bd3-9536-ad6d5feaec40",
   "metadata": {},
   "source": [
    "We will use PPO as the RL algorithm.  It's a good algorithm for many RL tasks, so we often use it in testing.  The `learn()` method will execute the learning cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc51af-d1ad-4032-94c6-8f4ebe058a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=0)\n",
    "model.learn(total_timesteps=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c431c595-7634-4a3c-b871-5ac2530aa789",
   "metadata": {},
   "source": [
    "Let's have a look at how the training turned out.  Load the data into a dataframe and plot it with HoloViews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b8807-14c0-4849-a6a1-f289ca3eff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_reward = pd.DataFrame(pd.to_numeric(pd.read_csv(\"tmp/monitor.csv\")[1:].reset_index()['index'])).reset_index()\n",
    "training_reward.rename(columns={'level_0':\"Episode\",'index':\"Reward\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f13b68-49fa-4b7c-af30-c58b58e09087",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_reward.hvplot(x=\"Episode\",y=\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f923f-ba77-448a-9d42-15ecaecc3a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "episode_reward = 0\n",
    "obs = env.reset()\n",
    "for _ in trange(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward \n",
    "    env.render()\n",
    "    if done:\n",
    "        reward_list.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        env.reset()\n",
    "env.env.viewer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef427065-594b-4edf-92b6-564a5e2da0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_to_plot = pd.DataFrame(reward_list)\n",
    "reward_to_plot.hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf443a-9156-4668-9306-a250cc15d588",
   "metadata": {
    "tags": []
   },
   "source": [
    "The CartPole environment is very simple.  It contains a cart that moves on a track, either to the left or to the right.  The environment has gravity, but not wind.  The agent is not evident in the environment.  We only know that 'something' pushes the cart one way or another to increase or decrease its velocity.  \n",
    "\n",
    "The environment provides a testing ground for an algorithm's ability to solve a fundamental control problem by choosing between only two actions, a force to the left or a force to the right.  Examples of control problems include automotive cruise control and the thermostat that controls a heating system.  The CartPole animation makes the environment a little more fun to use.\n",
    "\n",
    "During the RL cycle, the cart is pushed back and forth to keep the pole balanced.  If the pole falls, the learning episode is finished; the environment is reset and the agent tries again.  The agent retains its knowledge for each episode of learning, so it slowly improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1625eb8-7361-4ecf-8b9d-36c058fee0c8",
   "metadata": {},
   "source": [
    ">**To gain a better sense of how well the agent learns over time, try making at least three plots of episodes vs reward.**  Suggested time steps are 10,000, 25,000, and 50,000.  It might help to save each plot as an image then display each in a markdown file with `![plot name](path/to/image)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082e280c-78a2-4cca-b6e4-6dbbb16fa835",
   "metadata": {},
   "source": [
    "### State-Action Space\n",
    "The CartPole environment has a small state-action space.\n",
    "\n",
    "![CartPole](images/cartpole.jpg)\n",
    "\n",
    "State variables (observable variables):\n",
    "- cart position\n",
    "- cart velocity\n",
    "- pole angle\n",
    "- pole angular velocity\n",
    "\n",
    "Action variables: \n",
    "- push left\n",
    "- push right\n",
    "\n",
    "This simplicity helps make learning very fast, even on a small computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be418d0e-1762-48ba-a198-ea7029022245",
   "metadata": {},
   "source": [
    "### Reward\n",
    "The reward function for any project is critical to the performance and generalization of the model.  If the reward is too specific, the model will learn a very specific task.  If the reward is too ambiguous, the model will probably not learn much.\n",
    "\n",
    "For the CartPole problem, there are probably several effective reward functions that will work.  The [default reward function](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py#L135) simply gives a point at each time step if the pole is still standing (>24 degree pole angle). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1c376c-eaab-43d8-8c12-67ef57b23102",
   "metadata": {},
   "source": [
    "Here is a simplified version of the reward function (reward function 1):\n",
    "```python\n",
    "if pole_angle >= 24:\n",
    "    reward += 1.0\n",
    "else:\n",
    "    reward += 0.0\n",
    "```\n",
    "\n",
    "The longer the pole remains standing, the larger the reward.\n",
    "\n",
    ">**Can you think of an equally effective reward function for CartPole?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2034f-3a5b-4da8-b0e2-a9811ee37401",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "An equally effective reward function would be to give a point for every time step.  The longer it balances the pole, the higher the reward.  A better reward function might be a factor of the pole angle: reward += pole_angle/90\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4addce8d-431e-4e11-aec1-8162230404b6",
   "metadata": {},
   "source": [
    "Consider this reward function for CartPole (reward function 2):\n",
    "```python\n",
    "if pole_angle >= 80:\n",
    "    reward += 1.0\n",
    "else:\n",
    "    reward += 0.0\n",
    "```\n",
    "\n",
    ">Is this function better or worse than the function above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55596085-dcb4-4d11-a4da-db694851e81a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "This is tricky.  This reward function is generally not well designed.  The agent has no explicit incentive to keep the pole angle greater than 80, so the final model might not be very steady.  It could move back and forth quickly, always keeping the pole angle around 80 degrees.  \n",
    "    \n",
    "Also, raising the pole angle threshold to 80 degrees (reward function 2) might make learning slower.  Remember that the agent does not know anything about balancing poles when it first starts to learn.  If it rarely gets a reward when it first starts to learn (because it's hard to keep the pole above 80 degrees), it will take more time to learn.  \n",
    "    \n",
    "Let's think about why reward function 1 works well.  It gives the agent a point if the pole is at any angle above 24 degrees.  This angle is low enough that the agent will receive rewards even if the pole is falling down, and as long as the agent pushes the cart in the same direction, the pole will fall down more slowly, which produces more reward.\n",
    "    \n",
    "In practice, we know that the agent will eventually get very good at balancing the pole in a slow and steady manner.  It does not have an explicit reward for being steady, but the fact that it is always exploring, like pushing even when the pole is close to 90 degrees, is probably the reason it continues to improve when it is allowed to learn over many episodes.\n",
    "    \n",
    "Considering this discussion, is `reward += pole_angle/90` a better reward function?\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d072395-5f94-4008-ad80-dca3a5ea0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import registry, make, spec\n",
    "\n",
    "def register(id, *args, **kvargs):\n",
    "    if id in registry.env_specs:\n",
    "        return\n",
    "    else:\n",
    "        return gym.envs.registration.register(id, *args, **kvargs)\n",
    "\n",
    "register(id='MyCartPole-v1',\n",
    "         entry_point='MyCartPole:MyCartPoleEnv',\n",
    "         max_episode_steps=1000,\n",
    "         reward_threshold=2500.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457b0a4-6de7-411c-af34-7d2d51bae93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = gym.make(\"MyCartPole-v1\")\n",
    "env = Monitor(env, log_dir)\n",
    "model = PPO('MlpPolicy', env, verbose=0)\n",
    "model.learn(total_timesteps=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788199ab-be95-4e62-bd7b-e50920f1bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_reward = pd.DataFrame(pd.to_numeric(pd.read_csv(\"tmp/monitor.csv\")[1:].reset_index()['index'])).reset_index()\n",
    "training_reward.rename(columns={'level_0':\"Episode\",'index':\"Reward\"},inplace=True)\n",
    "training_reward.hvplot(x=\"Episode\",y=\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128f52d-1040-4eac-9498-3e82e2ce45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73469ebc-7608-4891-b8b9-a49c70becb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
