{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c88cdd",
   "metadata": {},
   "source": [
    "## Day 4, Part B: Creating Custom Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a60c5f-3997-4d03-9c6e-f4ded3ab6778",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "- How to build your own custom environment\n",
    "- How to connect environments to other simulations/platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6f2bf-7a6f-4a89-9510-b998bfc602b1",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "- **Simulation environment**: Notice that this is not the same as the python/conda environment.  The simulation environment is the simulated world where the reinforcement learning takes place.  It provides opportunities for an agent to learn and explore, and ideally provides challenges that aid in efficient learning.\n",
    "- **Agent (aka actor or policy)**: An entity in the simulation environment that performs actions.  The agent could be a person, a robot, a car, a thermostat, etc.\n",
    "- **State variable**: An observed variable in the simulation environment.  They can be coordinates of objects or entities, an amount of fuel in a tank, air temperature, wind speed, etc.\n",
    "- **Action variable**: An action that the agent can perform.  Examples: step forward, increase velocity to 552.5 knots, push object left with force of 212.3 N, etc.\n",
    "- **Reward**: A value given to the agent for doing something considered to be 'good'.  Reward is commonly assigned at each time step and cumulated during a learning episode.\n",
    "- **Episode**: A learning event consisting of multiple steps in which the agent can explore.  It starts with the unmodified environment and continues until the goal is achieved or something prevents further progress, such as a robot getting stuck in a hole.  Multiple episodes are typically run in loops until the model is fully trained.\n",
    "- **Model (aka policy or agent)**: An RL model is composed of the modeling architecture (e.g., neural network) and parameters or weights that define the unique behavior of the model.\n",
    "- **Policy (aka model or agent)**: The parameters of a model that encode the best choices to make in an environment.  The choices are not necessarily good ones until the model undergoes training.  The policy (or model) is the \"brain\" of the agent.\n",
    "- **Replay Buffer**: A place in memory to store state, action, reward and other variables describing environmental state transitions. It is effectively the agent's memory of past experiences.\n",
    "- **On-policy**: The value of the next action is determined using the current actor policy.\n",
    "- **Off-policy**: The value of the next action is determined by a function, such as a value function, instead of the current actor policy.\n",
    "- **Value function**: Function (typically a neural network) used to estimate the value, or expected reward, of an action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dea2ae-fc26-4887-92b7-6f8f1a3991e9",
   "metadata": {},
   "source": [
    "## More practical than the Ant environment\n",
    "\n",
    "![Ant Environment](./images/ant_env.jpg)\n",
    "\n",
    "Well.  Now you can take your ant off-the-shelf and make it do all sorts of fun tricks with modifications to the environment, the reward, and the training routine.  It's time to go make an army of virtual ants to do everything we need.  Oh - you had other, *actual*, applications?  Are you sure it can't be solved with a dancing ant?\n",
    "\n",
    "And that brings us to the pressing point: a virtual ant is well and fine, but we don't *actually* just want a fancy, simulated robo-ant.  What we want is to be able to hook up our new intelligent machinery to something we actually care about and that needs a boost from an RL model.\n",
    "\n",
    "Maybe the agent is a person, a vehicle, a robot, or whatever.  It could also have actions that we haven't used before, like pick up box and put down box. You can choose whatever state and action variables you need to accomplish a goal in the environment.\n",
    "\n",
    "The environment does not need to be a simulated world either.  RL models can control lots of things.  For example, Google uses RL to control its [data warehouse cooling system](https://blog.google/outreach-initiatives/environment/data-centers-get-fit-on-efficiency/).\n",
    "\n",
    "![Google Warehouse Cooling System](./images/GOOGLE_IDI_014.max-1000x1000.jpg)\n",
    "\n",
    "In this lesson, however, we will stick with an environment that is not too different from the one you've been working with for the sake of simplicy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996071d6-afca-4004-a267-332585631cc7",
   "metadata": {},
   "source": [
    "## Customize `SimpleEnv`\n",
    "\n",
    "Thankfully, the template to make your own environment is actually simpler than you'd think.  The details will only get as messy as your environment, so start with the general architecture and add features as you go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a9b9d5-d708-4aa4-920f-b8453e47036f",
   "metadata": {},
   "source": [
    "By building a new class off of `gym.Env` (subclassing) we get all the general machinery we need - we just have to define four things:\n",
    "- `__init__(self)`\n",
    "- `step(self, action)`\n",
    "- `render(self)`\n",
    "- `reset(self)`\n",
    "- Bonus optional: `seed(self, seed=None)`\n",
    "\n",
    "The great majority of your work will happen in `step()` - given the past `action` (usually an array) this is what takes your environment from state `t` to state `t+1`.  The `step()` is also in charge of determining the reward for that step from `t` to `t+1` and returning it. In a simple case, your `step()` may simply take the action components and modify the environment directly, or in more complicated cases, this is where you would call other components in your simulation engine to step forward (e.g. robotics, physics, etc. simulations).  If you were, for example, running a physics environment similar to Pybullet, then `step()` would pass the action and step-simulation commands over to the engine and have methods for receiving the new state, as well as calculating the reward.\n",
    "\n",
    "Take a look at a bare-bones setup below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efee71-ceec-4480-b1f0-7be368b0f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, gym.spaces, gym.utils, gym.utils.seeding\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SimpleEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_space = gym.spaces.box.Box(low=0,high=1, shape=(2,), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.box.Box(low=0,high=10, shape=(4,), dtype=np.float32)\n",
    "        self.reset()\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.state = np.array([action[0],action[0],action[1],action[1]], dtype=np.float32)\n",
    "        reward = 1\n",
    "        done = False\n",
    "        \n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.array([0,0,0,0], dtype=np.float32)\n",
    "        return self.state\n",
    "        \n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b93d23-f3e9-420c-8e56-5287b06e3ba6",
   "metadata": {},
   "source": [
    "If you were to take our usual TD3 implementation (similar to the code below), dump the code above into a file named `MySimpleEnv.py`, and change the registration to point to `file:class` it will run without error and dump 1000 as the episode reward over and over (reward = 1 for 1000 steps).  It's not terribly exciting, but it ran!\n",
    "\n",
    "Let's look more closely at `action_space` and `observation_space`.  These are defined by gym utility functions in `spaces`.  They define the dimension and type (discrete or continuous) of each variable.\n",
    "- It's up to the step function to make sense of those variables, and it is important to make sure values stay within the low/high ranges dictated by the definitions\n",
    "- For algorithms like TD3 that apply action noise, it's very important to understand the scale of each of the action values.  If you have one variable on the range [0,100] and another on [0,1], but the algorithm is basing the scale of noise off of the [0,100] variable, you may be completely washing out your second action variable with noise, unless you make modifications to the routine to make the action noise scaled per action.\n",
    "  \n",
    "Try to get a feel for what's happening in the action and observation spaces by examining some characteristics of them (**add some new cells and examine the observation space too**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca46ff0-0a32-424b-882d-a7526f69b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.box.Box(low=0,high=1, shape=(2,), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f55ad0-cdc6-4bfe-9563-b777a7d11d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f15276-2721-4553-890e-cdfab60fa188",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21433fe-3a01-409f-a8b9-eb2a0266966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space.high[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842388fc-e91f-46e7-a647-ca3894c3b7e8",
   "metadata": {},
   "source": [
    "In TD3 `main()`, they scale noise by `max_action = float(env.action_space.high[0])` so all action noise is based on the scale of the first action-space variable.\n",
    "\n",
    "Looking again at the SimpleEnv() class:\n",
    "\n",
    "- `step()` takes the action array and applies changes to the environment state with those action variables.\n",
    "- `step()` then returns that state in the range and dimension that the observation_space can handle.\n",
    "  - recall that not all information about the state of your environment needs to be passed back.  You can have placeholders and simulation states held by things external to the observation/state return.\n",
    "- Finally, `reset()` needs to take the environment state back to the starting conditions of your episode - just reset everything to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac998f12-bc91-4750-8b52-297452dd310e",
   "metadata": {},
   "source": [
    "## MyCustomEnv\n",
    "\n",
    "Let's now look at an environment with a tiny bit more meat on its bones - we've included `MyCustomEnv.py` in the `Course_Material` folder.  If you look at the code, you'll see it's not doing *that* much more than the SimpleEnv above.  The general idea in the environment is that there's an agent that needs to get to a point in (x,y)-space.  \n",
    "\n",
    "In `MyCustomEnv`, the agent has two state variables\n",
    "- Position\n",
    "- Heading\n",
    "\n",
    "And it has two actions:\n",
    "- Turn heading +/- 40 degrees\n",
    "- Throttle\n",
    "\n",
    "It's rewarded by getting closer to the target coordinates, similar to our ant, but without all the robotics and physics in the way - point your agent's heading, hit the throttle and it moves; no momentum, etc.\n",
    "\n",
    "If you run the code below (again, just the `main()` from TD3 with our new environment registered) it will train the agent to achieve the goal we defined in the reward. Note these two points: \n",
    "1. We can create environments that will actually solve, given the right inputs.\n",
    "2. This could have been solved by an extremely simple, hand-coded function.  Not everything needs to be RL... but it can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c47d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "import utils\n",
    "import TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs policy for X episodes and returns average reward\n",
    "# A fixed seed is used for the eval environment\n",
    "def eval_policy(policy, env_name, seed, eval_episodes=10):\n",
    "    eval_env = gym.make(env_name)\n",
    "    eval_env.seed(seed + 100)\n",
    "\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84acb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import registry, make, spec\n",
    "\n",
    "\n",
    "def register(id, *args, **kvargs):\n",
    "    if id in registry.env_specs:\n",
    "        return\n",
    "    else:\n",
    "        return gym.envs.registration.register(id, *args, **kvargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8863e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    id='MyCustomEnv-v0',\n",
    "    entry_point='MyCustomEnv:MyCustomEnvClass',\n",
    "    max_episode_steps=1000,\n",
    "    reward_threshold=2500.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = {\n",
    "            \"policy\" : \"TD3\",                  # Policy name (TD3, DDPG or OurDDPG)\n",
    "            \"env\" : \"MyCustomEnv-v0\",         # OpenAI gym environment name\n",
    "            \"seed\" : 0,                        # Sets Gym, PyTorch and Numpy seeds\n",
    "            \"start_timesteps\" : 25e3,          # Time steps initial random policy is used\n",
    "            \"eval_freq\" : 5e3,                 # How often (time steps) we evaluate\n",
    "            \"max_timesteps\" : 0.25e6,             # Max time steps to run environment\n",
    "            \"expl_noise\" : 0.1,                # Std of Gaussian exploration noise\n",
    "            \"batch_size\" : 256,                # Batch size for both actor and critic\n",
    "            \"discount\" : 0.99,                 # Discount factor\n",
    "            \"tau\" : 0.005,                     # Target network update rate\n",
    "            \"policy_noise\" : 0.2,              # Noise added to target policy during critic update\n",
    "            \"noise_clip\" : 0.5,                # Range to clip target policy noise\n",
    "            \"policy_freq\" : 2,                 # Frequency of delayed policy updates\n",
    "            \"save_model\" : \"store_true\",       # Save model and optimizer parameters\n",
    "            \"load_model\" : \"\",                 # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "           }\n",
    "\n",
    "    file_name = f\"{args['policy']}_{args['env']}_{args['seed']}_custom\"\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Policy: {args['policy']}, Env: {args['env']}, Seed: {args['seed']}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    if not os.path.exists(\"./rewards\"):\n",
    "        os.makedirs(\"./rewards\")\n",
    "    \n",
    "    if not os.path.exists(\"./results\"):\n",
    "        os.makedirs(\"./results\")\n",
    "\n",
    "    if args['save_model'] and not os.path.exists(\"./models\"):\n",
    "        os.makedirs(\"./models\")\n",
    "\n",
    "    env = gym.make(args['env'])\n",
    "    # Set seeds\n",
    "    env.seed(args['seed'])\n",
    "    env.action_space.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0] \n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    kwargs = {\n",
    "        \"state_dim\": state_dim,\n",
    "        \"action_dim\": action_dim,\n",
    "        \"max_action\": max_action,\n",
    "        \"discount\": args['discount'],\n",
    "        \"tau\": args['tau'],\n",
    "    }\n",
    "\n",
    "    # Initialize policy\n",
    "    if args['policy'] == \"TD3\":\n",
    "        # Target policy smoothing is scaled wrt the action scale\n",
    "        kwargs[\"policy_noise\"] = args['policy_noise'] * max_action\n",
    "        kwargs[\"noise_clip\"] = args['noise_clip'] * max_action\n",
    "        kwargs[\"policy_freq\"] = args['policy_freq']\n",
    "        policy = TD3.TD3(**kwargs)\n",
    "\n",
    "    if args['load_model'] != \"\":\n",
    "        policy_file = file_name if args['load_model'] == \"default\" else args['load_model']\n",
    "        policy.load(f\"./models/{policy_file}\")\n",
    "\n",
    "    replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "\n",
    "    # Evaluate untrained policy\n",
    "    evaluations = [eval_policy(policy, args['env'], args['seed'])]\n",
    "\n",
    "    state, done = env.reset(), False\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num = 0\n",
    "\n",
    "    with open('./rewards/Day4_cust.txt', 'w') as f:\n",
    "        for t in range(int(args['max_timesteps'])):\n",
    "\n",
    "            episode_timesteps += 1\n",
    "\n",
    "            # Select action randomly or according to policy\n",
    "            if t < args['start_timesteps']:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = (\n",
    "                    policy.select_action(np.array(state))\n",
    "                    + np.random.normal(0, max_action * args['expl_noise'], size=action_dim)\n",
    "                ).clip(-max_action, max_action)\n",
    "\n",
    "            # Perform action\n",
    "            next_state, reward, done, _ = env.step(action) \n",
    "            done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "            # Store data in replay buffer\n",
    "            replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Train agent after collecting sufficient data\n",
    "            if t >= args['start_timesteps']:\n",
    "                policy.train(replay_buffer, args['batch_size'])\n",
    "\n",
    "            if done: \n",
    "                # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "                print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "                print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\", file=f)\n",
    "                # Reset environment\n",
    "                state, done = env.reset(), False\n",
    "                episode_reward = 0\n",
    "                episode_timesteps = 0\n",
    "                episode_num += 1 \n",
    "\n",
    "            # Evaluate episode\n",
    "            if (t + 1) % args['eval_freq'] == 0:\n",
    "                evaluations.append(eval_policy(policy, args['env'], args['seed']))\n",
    "                np.save(f\"./results/{file_name}\", evaluations)\n",
    "                if args['save_model']: policy.save(f\"./models/{file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c2a21-504b-4f1f-8b3f-1c8e2d1eb064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376dddbe-4fe2-4e40-bb56-bdb8cc1aec3d",
   "metadata": {},
   "source": [
    "Our included `SingleAnalysis.ipynb` will plot your rewards over time.  We also modified `main()` above to dump the rewards info to file along the way, so if you've just run things, it should be there for you.  We also have the plotting code duplicated below, so you can run things here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea0201-d965-47a4-9277-7623847e5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas\n",
    "import hvplot.pandas\n",
    "\n",
    "t_steps = []\n",
    "reward_vals = []\n",
    "\n",
    "def build_plot(file_name):\n",
    "\n",
    "    f = open(f\"./rewards/{file_name}.txt\")\n",
    "    \n",
    "    for i in f:\n",
    "        if i[0] == \"T\":\n",
    "            a = str.split(i, \":\")\n",
    "            b = str.split(a[1], \" \")\n",
    "\n",
    "            t_steps.append(int(b[1]))\n",
    "            reward_vals.append(float(a[-1].strip()))\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191cd69-4dc5-4bb3-b2cd-7513056c11a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_plot(\"Day4_cust\")\n",
    "df = pandas.DataFrame({'Time_Steps':t_steps, 'Reward':reward_vals})\n",
    "\n",
    "df.hvplot(x='Time_Steps', y='Reward').opts(alpha=0.5, color='#8848ab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a46105-5c96-4129-ac83-306275aff5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
