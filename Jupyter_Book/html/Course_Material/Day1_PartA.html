
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Day 1, Part A: Introduction to reinforcement learning and research environments &#8212; Practical-RL</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Day 1, Part B: More on Reward Design" href="Day1_PartB.html" />
    <link rel="prev" title="Syllabus: Practical Reinforcement Learning Course" href="Practical_RL_Course_Intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Practical-RL</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../README.html">
   Readme
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Practical_RL_Course_Intro.html">
   Syllabus: Practical Reinforcement Learning Course
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Day 1, Part A: Introduction to reinforcement learning and research environments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day1_PartB.html">
   Day 1, Part B: More on Reward Design
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day1_PartC.html">
   Day 1, Part C: Faster Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day1_PartD.html">
   Day 1, Part D: Parking a Car with RL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day2_PartA.html">
   Day 2, Part A: Learning To Run
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day2_PartB.html">
   Day 2, Part B: TD3 Algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day3_PartA.html">
   Day 3, Part A: Modifying The Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day3_PartB.html">
   Day 3, Part B: Reward Shaping, Generalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day4_PartA.html">
   Day 4, Part A: Hyperparameter Tuning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day4_PartB.html">
   Day 4, Part B: Creating Custom Environments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day5_PartA.html">
   Day 5, Part A: Multi-Agent RL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day5_PartB.html">
   Day 5, Part B: Using Your Trained Policy
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Course_Material/Day1_PartA.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/Quansight/Practical-RL"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        <a class="edit-button" href="https://github.com/Quansight/Practical-RL/edit/main/Course_Material/Day1_PartA.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/Quansight/Practical-RL/main?urlpath=tree/Course_Material/Day1_PartA.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-goals">
   Learning goals
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#definitions">
   Definitions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-technical-overview-of-reinforcement-learning-rl">
   A Technical Overview of Reinforcement Learning (RL)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rl-cycle">
     RL Cycle
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simulation-environments">
     Simulation Environments
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#environment-example-cartpole">
       Environment example: CartPole
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#state-action-space">
     State-Action Space
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reward">
     Reward
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="day-1-part-a-introduction-to-reinforcement-learning-and-research-environments">
<h1>Day 1, Part A: Introduction to reinforcement learning and research environments<a class="headerlink" href="#day-1-part-a-introduction-to-reinforcement-learning-and-research-environments" title="Permalink to this headline">¶</a></h1>
<div class="section" id="learning-goals">
<h2>Learning goals<a class="headerlink" href="#learning-goals" title="Permalink to this headline">¶</a></h2>
<p>In addition to understanding principles of reinforcement learning (RL) and the tools we use to experiment in RL, you will learn several other things:</p>
<ul class="simple">
<li><p>Ability to use common terminology in RL</p></li>
<li><p>Understand the importance of each step of the RL cycle</p></li>
<li><p>How to install and run a model in an OpenAI Gym environment</p></li>
<li><p>Think carefully about reward functions, and discuss possible improvements</p></li>
</ul>
</div>
<div class="section" id="definitions">
<h2>Definitions<a class="headerlink" href="#definitions" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><strong>Simulation environment</strong>: Notice that this is not the same as the python/conda environment.  The simulation environment is the simulated world where the reinforcement learning takes place.  It provides opportunities for an agent to learn and explore, and ideally provides challenges that aid in efficient learning.</p></li>
<li><p><strong>Agent (aka actor or policy)</strong>: An entity in the simulation environment that performs actions.  The agent could be a person, a robot, a car, a thermostat, etc.</p></li>
<li><p><strong>State variable</strong>: An observed variable in the simulation environment.  They can be coordinates of objects or entities, an amount of fuel in a tank, air temperature, wind speed, etc.</p></li>
<li><p><strong>Action variable</strong>: An action that the agent can perform.  Examples: step forward, increase velocity to 552.5 knots, push object left with force of 212.3 N, etc.</p></li>
<li><p><strong>Reward</strong>: A value given to the agent for doing something considered to be ‘good’.  Reward is commonly assigned at each time step and cumulated during a learning episode.</p></li>
<li><p><strong>Episode</strong>: A learning event consisting of multiple steps in which the agent can explore.  It starts with the unmodified environment and continues until the goal is achieved or something prevents further progress, such as a robot getting stuck in a hole.  Multiple episodes are typically run in loops until the model is fully trained.</p></li>
<li><p><strong>Model (aka policy or agent)</strong>: An RL model is composed of the modeling architecture (e.g., neural network) and parameters or weights that define the unique behavior of the model.</p></li>
<li><p><strong>Policy (aka model or agent)</strong>: The parameters of a model that encode the best choices to make in an environment.  The choices are not necessarily good ones until the model undergoes training.  The policy (or model) is the “brain” of the agent.</p></li>
<li><p><strong>Replay Buffer</strong>: A place in memory to store state, action, reward and other variables describing environmental state transitions. It is effectively the agent’s memory of past experiences.</p></li>
</ul>
</div>
<div class="section" id="a-technical-overview-of-reinforcement-learning-rl">
<h2>A Technical Overview of Reinforcement Learning (RL)<a class="headerlink" href="#a-technical-overview-of-reinforcement-learning-rl" title="Permalink to this headline">¶</a></h2>
<p>For a less technical overview of RL, please read <a class="reference external" href="https://www.quansight.com/post/exploring-reinforcement-learning">the blog post</a>.</p>
<p>Learning a new skill can sometimes be daunting - maybe you read books, take a class, watch videos of people who already know - maybe you prefer to simply dive in and make a mess of things as you go, learning from the process. Similarly, there are a number of ways in which a computer model can learn a new skill. In the world of machine learning, RL takes the latter approach of diving right in and trying things out to see how they go.</p>
<div class="section" id="rl-cycle">
<h3>RL Cycle<a class="headerlink" href="#rl-cycle" title="Permalink to this headline">¶</a></h3>
<p>RL is used to train a model to perform one or more tasks.  It is often applied when we do not have data that can be used to train a supervised learning model.  Instead, we set up a simulated environment and give an agent the ability to explore and learn about the environment.  The agent is a representation of something in the environment that can perform actions.  It could be a person, a vehicle, a light switch, a control circuit, etc.  In the diagram below, it is a robot.</p>
<p>At first, the agent knows nothing about the environment, but it can make observations of the state variables in the environment, such as locations and quantities of things, temperature, time, and so on.  We can give the agent a reward for doing something we consider good, and it will continue to seek rewards. In general, the more it explores and experiments with actions in the environment, the better it learns.</p>
<p><img alt="Reinforcement Learning Cycle" src="../_images/Reinforcement-learning-diagram-01.png" /></p>
<p>The cycle typically completes a loop for each time step in the simulation.</p>
<ol class="simple">
<li><p>The model (part of the agent in this diagram) outputs an action vector</p></li>
<li><p>The agent performs the action in the environment</p></li>
<li><p>The environment usually changes</p></li>
<li><p>The state variables and a reward value is passed to the model</p></li>
<li><p>The model takes the new state observations as input and outputs a new action vector for the next time step</p></li>
</ol>
<p>Early learning can be comical as the agent stumbles around trying to learn, but as learning proceeds, the agent begins to develop better skills, and if given enough opportunity to learn, it can find interesting and innovative solutions.</p>
<p>Below, see the basic ant trained to only 50k steps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">IPython.display</span> <span class="k">as</span> <span class="nn">ipd</span>
<span class="n">ipd</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="s2">&quot;images/base_ant_at50k.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And at 2M training steps</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ipd</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="s2">&quot;../animations/base_ant.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The RL cycle diagram above is relatively simple.  At the core of the agent is a model, essentially the brain of the agent.  The agent also provides a way to interact with the environment, so we often associate observations and actions with it.  The diagram below attempts to define the agent more explicitly.  Notice that the policy is actually the set of parameters (weights) of the network.  The network inputs are the state variables, and the outputs are the action variables.</p>
<p><img alt="Agent Components" src="../_images/rl_agent_huang_et_al_2019.png" /></p>
<p><a class="reference external" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8742652">Huang et al., 2019</a></p>
</div>
<div class="section" id="simulation-environments">
<h3>Simulation Environments<a class="headerlink" href="#simulation-environments" title="Permalink to this headline">¶</a></h3>
<p>In general, a simulated environment should be as close to the real environment as possible without making the simulation overly complex.  This requires a balance between simulation details and computational efficiency, which is specific to each problem.  Each problem has different challenges that the agent must overcome and the environement will need to be a good place for the agent to learn.</p>
<p>Most research and teaching in RL is done with simulated environments.  They also provide common ground for benchmarking a new algorithm and showing how well it performs relative to existing algorithms.  Several libraries of environments are available to do RL research, such as <a class="reference external" href="https://gym.openai.com/envs/">OpenAI Gym</a> and <a class="reference external" href="https://deepmind.com/blog/announcements/mujoco">MuJoCo</a>.  They commonly require a Physics engine, which is a way to represent Physics in a computer simulation.  These engines enable things like gravity, friction, and so on.</p>
<p>For this course, we will use OpenAI Gym environments paired with the <a class="reference external" href="https://github.com/bulletphysics/bullet3">PyBullet3</a> Physics engine where needed (some environments are self-contained).  We chose these because they are free, easy to set up, and we can modify the reward function, something that is important in RL work.</p>
<p>Let’s look at one of these environments.</p>
<div class="section" id="environment-example-cartpole">
<h4>Environment example: CartPole<a class="headerlink" href="#environment-example-cartpole" title="Permalink to this headline">¶</a></h4>
<p>The <a class="reference external" href="https://gym.openai.com/envs/CartPole-v1/">CartPole environment</a> runs quickly and provides a simple case for discussion.  It also does not need an external physics engine.  This description is from the Gym website:</p>
<blockquote>
<div><p>A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over.</p>
</div></blockquote>
<p>Here is a gif showing a trained CartPole model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ipd</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="s2">&quot;images/cart_pole.gif&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To make our own model, we’ll need to import <code class="docutils literal notranslate"><span class="pre">gym</span></code> and several classes and methods from the <code class="docutils literal notranslate"><span class="pre">stable_baselines</span></code> library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">stable_baselines3</span> <span class="kn">import</span> <span class="n">PPO</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.monitor</span> <span class="kn">import</span> <span class="n">Monitor</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.vec_env</span> <span class="kn">import</span> <span class="n">DummyVecEnv</span><span class="p">,</span> <span class="n">SubprocVecEnv</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.env_util</span> <span class="kn">import</span> <span class="n">make_vec_env</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.utils</span> <span class="kn">import</span> <span class="n">set_random_seed</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">trange</span>
<span class="kn">import</span> <span class="nn">hvplot.pandas</span>  <span class="c1"># This adds HoloViews plotting capability directly from a Pandas dataframe</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
</div>
</div>
<p>Gym makes it easy to make an environment.  First we’ll set up logging so that we can more easily track progress of learning, and then we’ll create the environment.  Notice that we are wrapping the environment with the <code class="docutils literal notranslate"><span class="pre">Monitor</span></code> class to track the data that is generated during learning.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_dir</span> <span class="o">=</span> <span class="s2">&quot;tmp/&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">Monitor</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We will use PPO as the RL algorithm.  It’s a good algorithm for many RL tasks, so we often use it in testing.  The <code class="docutils literal notranslate"><span class="pre">learn()</span></code> method will execute the learning cycle.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">PPO</span><span class="p">(</span><span class="s1">&#39;MlpPolicy&#39;</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">25000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s have a look at how the training turned out.  Load the data into a dataframe and plot it with HoloViews.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">training_reward</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;tmp/monitor.csv&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()[</span><span class="s1">&#39;index&#39;</span><span class="p">]))</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="n">training_reward</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;level_0&#39;</span><span class="p">:</span><span class="s2">&quot;Episode&quot;</span><span class="p">,</span><span class="s1">&#39;index&#39;</span><span class="p">:</span><span class="s2">&quot;Reward&quot;</span><span class="p">},</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">training_reward</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;Episode&quot;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s2">&quot;Reward&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">reward_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">action</span><span class="p">,</span> <span class="n">_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
    <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">reward</span> 
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">reward_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_reward</span><span class="p">)</span>
        <span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">viewer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">reward_to_plot</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">reward_list</span><span class="p">)</span>
<span class="n">reward_to_plot</span><span class="o">.</span><span class="n">hvplot</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The CartPole environment is very simple.  It contains a cart that moves on a track, either to the left or to the right.  The environment has gravity, but not wind.  The agent is not evident in the environment.  We only know that ‘something’ pushes the cart one way or another to increase or decrease its velocity.</p>
<p>The environment provides a testing ground for an algorithm’s ability to solve a fundamental control problem by choosing between only two actions, a force to the left or a force to the right.  Examples of control problems include automotive cruise control and the thermostat that controls a heating system.  The CartPole animation makes the environment a little more fun to use.</p>
<p>During the RL cycle, the cart is pushed back and forth to keep the pole balanced.  If the pole falls, the learning episode is finished; the environment is reset and the agent tries again.  The agent retains its knowledge for each episode of learning, so it slowly improves.</p>
<blockquote>
<div><p><strong>To gain a better sense of how well the agent learns over time, try making at least three plots of episodes vs reward.</strong>  Suggested time steps are 10,000, 25,000, and 50,000.  It might help to save each plot as an image then display each in a markdown file with <code class="docutils literal notranslate"><span class="pre">![plot</span> <span class="pre">name](path/to/image)</span></code>.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="state-action-space">
<h3>State-Action Space<a class="headerlink" href="#state-action-space" title="Permalink to this headline">¶</a></h3>
<p>The CartPole environment has a small state-action space.</p>
<p><img alt="CartPole" src="../_images/cartpole.jpg" /></p>
<p>State variables (observable variables):</p>
<ul class="simple">
<li><p>cart position, x</p></li>
<li><p>cart velocity, v</p></li>
<li><p>pole angle, <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
<li><p>pole angular velocity, <span class="math notranslate nohighlight">\(\omega\)</span></p></li>
</ul>
<p>Action variables:</p>
<ul class="simple">
<li><p>push left</p></li>
<li><p>push right</p></li>
</ul>
<p>This simplicity helps make learning very fast, even on a small computer.</p>
</div>
<div class="section" id="reward">
<h3>Reward<a class="headerlink" href="#reward" title="Permalink to this headline">¶</a></h3>
<p>The reward function for any project is critical to the performance and generalization of the model.  If the reward is too specific, the model will learn a very specific task.  If the reward is too ambiguous, the model will probably not learn much.</p>
<p>For the CartPole problem, there are probably several effective reward functions that will work.  The <a class="reference external" href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py#L135">default reward function</a> simply gives a point at each time step if the pole is still standing (&gt;24 degree pole angle).</p>
<p>Here is a simplified version of the reward function (reward function 1):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">pole_angle</span> <span class="o">&gt;=</span> <span class="mi">24</span><span class="p">:</span>
    <span class="n">reward</span> <span class="o">+=</span> <span class="mf">1.0</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">reward</span> <span class="o">+=</span> <span class="mf">0.0</span>
</pre></div>
</div>
<p>The longer the pole remains standing, the larger the reward.</p>
<blockquote>
<div><p><strong>Can you think of an equally effective reward function for CartPole?</strong></p>
</div></blockquote>
<details>
<summary>Click to reveal answer</summary>
An equally effective reward function might be to give a point for every time step.  The longer it balances the pole, the higher the reward.  A better reward function might be a factor of the pole angle: reward += pole_angle/90
</details>
<br><p>Consider this reward function for CartPole (reward function 2):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">pole_angle</span> <span class="o">&gt;=</span> <span class="mi">80</span><span class="p">:</span>
    <span class="n">reward</span> <span class="o">+=</span> <span class="mf">1.0</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">reward</span> <span class="o">+=</span> <span class="mf">0.0</span>
</pre></div>
</div>
<blockquote>
<div><p>Is this function better or worse than the function above?</p>
</div></blockquote>
<details>
<summary>Click to reveal answer</summary>
This is tricky.  This reward function is generally not well designed.  The agent has no explicit incentive to keep the pole angle greater than 80, so the final model might not be very steady.  It could move back and forth quickly, always keeping the pole angle around 80 degrees.  
<p>Also, raising the pole angle threshold to 80 degrees (reward function 2) might make learning slower.  Remember that the agent does not know anything about balancing poles when it first starts to learn.  If it rarely gets a reward when it first starts to learn (because it’s hard to keep the pole above 80 degrees), it will take more time to learn.</p>
<p>Let’s think about why reward function 1 works well.  It gives the agent a point if the pole is at any angle above 24 degrees.  This angle is low enough that the agent will receive rewards even if the pole is falling down, and as long as the agent pushes the cart in the same direction, the pole will fall down more slowly, which produces more reward.</p>
<p>In practice, we know that the agent will eventually get very good at balancing the pole in a slow and steady manner.  It does not have an explicit reward for being steady, but the fact that it is always exploring, like pushing even when the pole is close to 90 degrees, is probably the reason it continues to improve when it is allowed to learn over many episodes.</p>
<p>Considering this discussion, is <code class="docutils literal notranslate"><span class="pre">reward</span> <span class="pre">+=</span> <span class="pre">pole_angle/90</span></code> a better reward function?</p>
</details>
<br><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Course_Material"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="Practical_RL_Course_Intro.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Syllabus: Practical Reinforcement Learning Course</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="Day1_PartB.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Day 1, Part B: More on Reward Design</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By tonyfast<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>