{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c88cdd",
   "metadata": {},
   "source": [
    "## Day 3, Part A: Modifying The Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a60c5f-3997-4d03-9c6e-f4ded3ab6778",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "- How to modify the Ant environment to accommodate new goals\n",
    "- How and why to subclass a class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6f2bf-7a6f-4a89-9510-b998bfc602b1",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "- **Simulation environment**: Notice that this is not the same as the python/conda environment.  The simulation environment is the simulated world where the reinforcement learning takes place.  It provides opportunities for an agent to learn and explore, and ideally provides challenges that aid in efficient learning.\n",
    "- **Agent (aka actor or policy)**: An entity in the simulation environment that performs actions.  The agent could be a person, a robot, a car, a thermostat, etc.\n",
    "- **State variable**: An observed variable in the simulation environment.  They can be coordinates of objects or entities, an amount of fuel in a tank, air temperature, wind speed, etc.\n",
    "- **Action variable**: An action that the agent can perform.  Examples: step forward, increase velocity to 552.5 knots, push object left with force of 212.3 N, etc.\n",
    "- **Reward**: A value given to the agent for doing something considered to be 'good'.  Reward is commonly assigned at each time step and cumulated during a learning episode.\n",
    "- **Episode**: A learning event consisting of multiple steps in which the agent can explore.  It starts with the unmodified environment and continues until the goal is achieved or something prevents further progress, such as a robot getting stuck in a hole.  Multiple episodes are typically run in loops until the model is fully trained.\n",
    "- **Model (aka policy or agent)**: An RL model is composed of the modeling architecture (e.g., neural network) and parameters or weights that define the unique behavior of the model.\n",
    "- **Policy (aka model or agent)**: The parameters of a model that encode the best choices to make in an environment.  The choices are not necessarily good ones until the model undergoes training.  The policy (or model) is the \"brain\" of the agent.\n",
    "- **Replay Buffer**: A place in memory to store state, action, reward and other variables describing environmental state transitions. It is effectively the agent's memory of past experiences.\n",
    "- **On-policy**: The value of the next action is determined using the current actor policy.\n",
    "- **Off-policy**: The value of the next action is determined by a function, such as a value function, instead of the current actor policy.\n",
    "- **Value function**: Function (typically a neural network) used to estimate the value, or expected reward, of an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad0382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Image(\"../animations/stuck_ant.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89b319f",
   "metadata": {},
   "source": [
    "As it turns out, the way that Ant is set up, it learns to run in the positive-x direction... and ONLY that direction.  It was never given any opportunity to have any other experiences to learn from, and so it didn't.  The result of the ant running off to the right is very effective at that one, specific, thing, but it's completely fragile to any other request or changes to the environment.\n",
    "\n",
    "Today we will think about how to make Ant more useful.  We will start with the idea of changing the environment, then move on the reward function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247d540-af4a-443a-a130-50d773a7e819",
   "metadata": {},
   "source": [
    "## Considerations when modifying an environment\n",
    "\n",
    "The Ant environment provides a good example for discussion.  The environment has the usual physical attributes, like gravity, but it is a flat, smooth world with no obstacles. If we want to train the Ant to do real-world tasks (using a robot version of Ant), we would need an improved learning environment.\n",
    "\n",
    "Like any analysis, it will help to define a goal or goals.  What should Ant accomplish?  The default goal is to walk to the right (along x) as fast as it can.  This can be very helpful as a testing environment, but what if we want Ant to perform other tasks?  It needs an environment that will help it learn across various tasks.\n",
    "- The goal needs to be challenging yet tractable (don't expect Ant to fly or do math)\n",
    "- Consider essential tasks in a real-world situation that can be included in the environment.  For example, does it need to navigate on uneven topography or around obstacles?\n",
    "\n",
    "Ant is designed for mobility, so obvious choices for a new goal are related to motion, direction, paths, and destinations.  Possible new goals:\n",
    "- Walk in opposite direction (not very useful)\n",
    "- Walk in specified direction (better) - if we keep giving it new directions to aim for, it can be more useful.\n",
    "- Go to coordinate (much better) - if we keep passing it coordinates as breadcrumbs, it could travel along complex paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f820dda-6307-42f3-8480-09eaa66ee311",
   "metadata": {},
   "source": [
    "![Ant Goals](./images/ant_goals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9872b7d-163a-4ae1-b738-74878af385ea",
   "metadata": {},
   "source": [
    "### The goal is not the same as the reward\n",
    "They are closely related though.  The reward helps to define how the agent accomplishes the goal.  When changing the goal of the agent, we usually need to adjust the reward.\n",
    "\n",
    "Example of each:\n",
    "- Goal: Get to target coordinate\n",
    "- Reward: Earn 1 point when agent coordinate is within x distance of target coordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6518f6-28ec-4608-8ff3-1dcf1fdf5a1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modify environment to accomodate new goal\n",
    "\n",
    "In Python, we can use subclassing to modify existing code, like the Ant environment.  Subclassing allows us to inherit the properties and methods of a class and then modify them by overriding the ones that need to change.  The class we inherit usually has a common set of characteristics, and each subclass is a more specialized version of it.\n",
    "\n",
    "By analogy, we can say that `Vehicle` defines a base class, `Car` defines a subclass of `Vehicle`, and `VW` defines a subclass of `Car`.  To define a `VW`, we would have a structure like this:\n",
    "\n",
    "![Subclassing](./images/subclassing_basic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbbdf9",
   "metadata": {},
   "source": [
    "Following the goal of moving to a new coordinate, lets look at how we might modify the environment code to make it a bit more flexible.\n",
    "\n",
    "Of course, you can define the entirety of your environment, including actors, steps, rewards, etc., but that's a lot of work if you don't need to.  Instead, let's subclass the things we want to modify.\n",
    "\n",
    "In our repository, if you look at [override_ant.py](https://github.com/dillonroach/TD3/blob/master/override_ant.py) we take all the pieces from [the original Ant](https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/gym/pybullet_envs) and give new names to things, while also exposing key functions in each piece we might want to modify.  The subclassing here gets messy, so here's a simplified rundown of what inherits from what:\n",
    "\n",
    "```python\n",
    "class MyAntBulletEnv(WalkerBaseBulletEnv): # <- this is our entry point to the new environment\n",
    "    self.robot = MyAnt()\n",
    "                   \\\\\\\n",
    "             class MyAnt(MyWalkerBase): # MyAntBulletEnv sets the robot as MyAnt\n",
    "                              \\\\\\\n",
    "                    class MyWalkerBase(WalkerBase): #MyAnt subclasses MyWalkerBase, itself from WalkerBase\n",
    "\n",
    "                        def init():\n",
    "                             ...\n",
    "                        def step():\n",
    "                             ...\n",
    "```\n",
    "\n",
    "If you take a look at `MyWalkerBase()` and compare to `WalkerBase()` from [the original](https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/robot_locomotors.py) we've copied over `init()` and `step()` as they were written, then tweaked small things.  In this first go, we've only used this to set the robot target x and y to a new location: 20m, 20m.\n",
    "\n",
    "In order to now use this new, modified environment, we simply add some registration boilerplate to our notebook:\n",
    "\n",
    "```python\n",
    "from gym.envs.registration import registry, make, spec\n",
    "\n",
    "\n",
    "def register(id, *args, **kvargs):\n",
    "    if id in registry.env_specs:\n",
    "        return\n",
    "    else:\n",
    "        return gym.envs.registration.register(id, *args, **kvargs)\n",
    "    \n",
    "register(\n",
    "    id='MyAntBulletEnv-v0',\n",
    "    entry_point='override_ant:MyAntBulletEnv',\n",
    "    max_episode_steps=1000,\n",
    "    reward_threshold=2500.0\n",
    ")\n",
    "```\n",
    "And then set `\"env\" : \"MyAntBulletEnv-v0\",` instead of the original, in the arguments dictionary.  You can see an example of this change in [TD3notebook-MyAnt.ipynb](https://github.com/dillonroach/TD3/blob/master/TD3notebook-MyAnt.ipynb).  If you run that notebook, you'll end up with an ant that runs to (20,20) instead of to the right for a kilometer.\n",
    "\n",
    "We also include the code here below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c47d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "import utils\n",
    "import TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs policy for X episodes and returns average reward\n",
    "# A fixed seed is used for the eval environment\n",
    "def eval_policy(policy, env_name, seed, eval_episodes=10):\n",
    "    eval_env = gym.make(env_name)\n",
    "    eval_env.seed(seed + 100)\n",
    "\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84acb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import registry, make, spec\n",
    "\n",
    "\n",
    "def register(id, *args, **kvargs):\n",
    "    if id in registry.env_specs:\n",
    "        return\n",
    "    else:\n",
    "        return gym.envs.registration.register(id, *args, **kvargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8863e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    id='MyAntBulletEnv-v0',\n",
    "    entry_point='override_ant:MyAntBulletEnv',\n",
    "    max_episode_steps=1000,\n",
    "    reward_threshold=2500.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = {\n",
    "            \"policy\" : \"TD3\",                  # Policy name (TD3, DDPG or OurDDPG)\n",
    "            \"env\" : \"MyAntBulletEnv-v0\",         # OpenAI gym environment name\n",
    "            \"seed\" : 0,                        # Sets Gym, PyTorch and Numpy seeds\n",
    "            \"start_timesteps\" : 25e3,          # Time steps initial random policy is used\n",
    "            \"eval_freq\" : 5e3,                 # How often (time steps) we evaluate\n",
    "            \"max_timesteps\" : 0.8e6,             # Max time steps to run environment\n",
    "            \"expl_noise\" : 0.1,                # Std of Gaussian exploration noise\n",
    "            \"batch_size\" : 256,                # Batch size for both actor and critic\n",
    "            \"discount\" : 0.99,                 # Discount factor\n",
    "            \"tau\" : 0.005,                     # Target network update rate\n",
    "            \"policy_noise\" : 0.2,              # Noise added to target policy during critic update\n",
    "            \"noise_clip\" : 0.5,                # Range to clip target policy noise\n",
    "            \"policy_freq\" : 2,                 # Frequency of delayed policy updates\n",
    "            \"save_model\" : \"store_true\",       # Save model and optimizer parameters\n",
    "            \"load_model\" : \"\",                 # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "           }\n",
    "\n",
    "    file_name = f\"{args['policy']}_{args['env']}_{args['seed']}\"\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Policy: {args['policy']}, Env: {args['env']}, Seed: {args['seed']}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    if not os.path.exists(\"./results\"):\n",
    "        os.makedirs(\"./results\")\n",
    "\n",
    "    if args['save_model'] and not os.path.exists(\"./models\"):\n",
    "        os.makedirs(\"./models\")\n",
    "\n",
    "    env = gym.make(args['env'])\n",
    "\n",
    "    # Set seeds\n",
    "    env.seed(args['seed'])\n",
    "    env.action_space.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0] \n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    kwargs = {\n",
    "        \"state_dim\": state_dim,\n",
    "        \"action_dim\": action_dim,\n",
    "        \"max_action\": max_action,\n",
    "        \"discount\": args['discount'],\n",
    "        \"tau\": args['tau'],\n",
    "    }\n",
    "\n",
    "    # Initialize policy\n",
    "    if args['policy'] == \"TD3\":\n",
    "        # Target policy smoothing is scaled wrt the action scale\n",
    "        kwargs[\"policy_noise\"] = args['policy_noise'] * max_action\n",
    "        kwargs[\"noise_clip\"] = args['noise_clip'] * max_action\n",
    "        kwargs[\"policy_freq\"] = args['policy_freq']\n",
    "        policy = TD3.TD3(**kwargs)\n",
    "\n",
    "    if args['load_model'] != \"\":\n",
    "        policy_file = file_name if args['load_model'] == \"default\" else args['load_model']\n",
    "        policy.load(f\"./models/{policy_file}\")\n",
    "\n",
    "    replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "\n",
    "    # Evaluate untrained policy\n",
    "    evaluations = [eval_policy(policy, args['env'], args['seed'])]\n",
    "\n",
    "    state, done = env.reset(), False\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num = 0\n",
    "\n",
    "    for t in range(int(args['max_timesteps'])):\n",
    "\n",
    "        episode_timesteps += 1\n",
    "\n",
    "        # Select action randomly or according to policy\n",
    "        if t < args['start_timesteps']:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = (\n",
    "                policy.select_action(np.array(state))\n",
    "                + np.random.normal(0, max_action * args['expl_noise'], size=action_dim)\n",
    "            ).clip(-max_action, max_action)\n",
    "\n",
    "        # Perform action\n",
    "        next_state, reward, done, _ = env.step(action) \n",
    "        done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "        # Store data in replay buffer\n",
    "        replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Train agent after collecting sufficient data\n",
    "        if t >= args['start_timesteps']:\n",
    "            policy.train(replay_buffer, args['batch_size'])\n",
    "\n",
    "        if done: \n",
    "            # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "            print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "            # Reset environment\n",
    "            state, done = env.reset(), False\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1 \n",
    "\n",
    "        # Evaluate episode\n",
    "        if (t + 1) % args['eval_freq'] == 0:\n",
    "            evaluations.append(eval_policy(policy, args['env'], args['seed']))\n",
    "            np.save(f\"./results/{file_name}\", evaluations)\n",
    "            if args['save_model']: policy.save(f\"./models/{file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f10aab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Image(\"../animations/diagon_anty.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a5e4b-5620-49ee-9933-8eb275dad19c",
   "metadata": {},
   "source": [
    "The Ant learned to get to the new coordinates (20, 20).  But, as you can see in the animation, it learned to walk in a different way, with two legs to the side, one in front, and one in back.  With small adjustments to the initial orientation of Ant, the walking style can be made more efficient, or we could let it continue learning and it should work out a more efficient style.\n",
    "\n",
    "Now that we've had success teaching Ant to do something beyond its default behavior, let's continue to work toward helping Ant to learn to go to any coordinate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
