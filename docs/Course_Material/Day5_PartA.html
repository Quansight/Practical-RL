
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Day 5, Part A: Multi-Agent RL &#8212; Practical-RL</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Day 5, Part B: Using Your Trained Policy" href="Day5_PartB.html" />
    <link rel="prev" title="Day 4, Part B: Creating Custom Environments" href="Day4_PartB.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Practical-RL</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../README.html">
   Readme
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Practical_RL_Course_Intro.html">
   Syllabus: Practical Reinforcement Learning Course
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day1_PartA.html">
   Day 1, Part A: Introduction to reinforcement learning and research environments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day1_PartB.html">
   Day 1, Part B: More on Reward Design
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day1_PartC.html">
   Day 1, Part C: Faster Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day1_PartD.html">
   Day 1, Part D: Parking a Car with RL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day2_PartA.html">
   Day 2, Part A: Learning To Run
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day2_PartB.html">
   Day 2, Part B: TD3 Algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day3_PartA.html">
   Day 3, Part A: Modifying The Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day3_PartB.html">
   Day 3, Part B: Reward Shaping, Generalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day4_PartA.html">
   Day 4, Part A: Hyperparameter Tuning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day4_PartB.html">
   Day 4, Part B: Creating Custom Environments
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Day 5, Part A: Multi-Agent RL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Day5_PartB.html">
   Day 5, Part B: Using Your Trained Policy
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Course_Material/Day5_PartA.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/Quansight/Practical-RL"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        <a class="edit-button" href="https://github.com/Quansight/Practical-RL/edit/main/Course_Material/Day5_PartA.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/Quansight/Practical-RL/main?urlpath=tree/Course_Material/Day5_PartA.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Day 5, Part A: Multi-Agent RL
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-goals">
   Learning goals
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#definitions">
   Definitions
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-agent">
   Multi-Agent
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="day-5-part-a-multi-agent-rl">
<h1>Day 5, Part A: Multi-Agent RL<a class="headerlink" href="#day-5-part-a-multi-agent-rl" title="Permalink to this headline">¶</a></h1>
</div>
<div class="section" id="learning-goals">
<h1>Learning goals<a class="headerlink" href="#learning-goals" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>What is MARL</p></li>
<li><p>Frameworks and considerations</p></li>
</ul>
</div>
<div class="section" id="definitions">
<h1>Definitions<a class="headerlink" href="#definitions" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><strong>Simulation environment</strong>: Notice that this is not the same as the python/conda environment.  The simulation environment is the simulated world where the reinforcement learning takes place.  It provides opportunities for an agent to learn and explore, and ideally provides challenges that aid in efficient learning.</p></li>
<li><p><strong>Agent (aka actor or policy)</strong>: An entity in the simulation environment that performs actions.  The agent could be a person, a robot, a car, a thermostat, etc.</p></li>
<li><p><strong>State variable</strong>: An observed variable in the simulation environment.  They can be coordinates of objects or entities, an amount of fuel in a tank, air temperature, wind speed, etc.</p></li>
<li><p><strong>Action variable</strong>: An action that the agent can perform.  Examples: step forward, increase velocity to 552.5 knots, push object left with force of 212.3 N, etc.</p></li>
<li><p><strong>Reward</strong>: A value given to the agent for doing something considered to be ‘good’.  Reward is commonly assigned at each time step and cumulated during a learning episode.</p></li>
<li><p><strong>Episode</strong>: A learning event consisting of multiple steps in which the agent can explore.  It starts with the unmodified environment and continues until the goal is achieved or something prevents further progress, such as a robot getting stuck in a hole.  Multiple episodes are typically run in loops until the model is fully trained.</p></li>
<li><p><strong>Model (aka policy or agent)</strong>: An RL model is composed of the modeling architecture (e.g., neural network) and parameters or weights that define the unique behavior of the model.</p></li>
<li><p><strong>Policy (aka model or agent)</strong>: The parameters of a model that encode the best choices to make in an environment.  The choices are not necessarily good ones until the model undergoes training.  The policy (or model) is the “brain” of the agent.</p></li>
<li><p><strong>Replay Buffer</strong>: A place in memory to store state, action, reward and other variables describing environmental state transitions. It is effectively the agent’s memory of past experiences.</p></li>
<li><p><strong>On-policy</strong>: The value of the next action is determined using the current actor policy.</p></li>
<li><p><strong>Off-policy</strong>: The value of the next action is determined by a function, such as a value function, instead of the current actor policy.</p></li>
<li><p><strong>Value function</strong>: Function (typically a neural network) used to estimate the value, or expected reward, of an action.</p></li>
</ul>
</div>
<div class="section" id="multi-agent">
<h1>Multi-Agent<a class="headerlink" href="#multi-agent" title="Permalink to this headline">¶</a></h1>
<p>If reinforcement learning is an area of ongoing research, then multi-agent RL (MARL) is its latest frontier.  As such, there’s less we can definitively say about optimal implementations and resources for it.  What we can do is describe the key concepts, some places to look for information, and some of the platforms/frameworks available.</p>
<p>First thing to note: there are two types of multi-agent RL concepts that are prominent today:</p>
<ul class="simple">
<li><p>Multiple agents in one or more environments sharing knowledge as they explore</p></li>
<li><p>Multiple agents working together to accomplish one or more goals</p></li>
</ul>
<p>The first one is a method for multiplying the algorithm’s ability to sample the environment space (via vectorization, parallel environments, or many agents in a single environment that pool learning); an example of this is NVIDIA’s <a class="reference external" href="https://sites.google.com/view/isaacgym-nvidia">isaac gym</a> - nothing like seeing another group spit out 32 million time-steps in 75sec to make our TD3 ant feel silly for spending all that time on its own.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Audio</span><span class="p">,</span><span class="n">Image</span><span class="p">,</span> <span class="n">YouTubeVideo</span>
<span class="nb">id</span> <span class="o">=</span> <span class="s1">&#39;Jq9NEpZOpik&#39;</span>
<span class="n">YouTubeVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="nb">id</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span><span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The main challenges in this first type of multi-agent RL are usually dev-ops questions of scaling and collecting/communicating information correctly and efficiently.</p>
<p>The second type of MARL is the harder bit - and it is harder - that is, many <em>different</em> agents learning to play individual parts in a larger group.  In this case, we have to consider how multiple agents learn in the context of environment stationarity (whether changes in the environment caused by one agent affect the actions taken by another, independent agent).  If each agent is learning, then each agent’s behavior is changing over time.  From one agent’s perspective, all other agents can be considered part of the environment, and if those agents are changing behavior over time, the environment is changing over time.  The time needed to learn in this kind of rapidly changing environment increases greatly.</p>
<p>Another way to view this problem is that the policy becomes a moving target.  The goal is for the agent (policy) to learn to anticipate all of the state-action transitions in the environment.  However, if those transitions are changing over time, the policy will have a difficult time converging on a solution that appears to be changing as well.</p>
<p>Notice that in the first type of “pooled learning” MARL, each agent usually has its own environment to explore (vectorized or parallel environments).  In those cases, the effect of actions taken by an agent are generally easier for the algorithm to learn.</p>
<p>So instead of relying on the same algorithms used to train single angents, MARL algorithms are being developed to handle the new complexities of this kind of RL.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">id</span><span class="o">=</span><span class="s1">&#39;HCSm0kVolqI&#39;</span>
<span class="n">YouTubeVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="nb">id</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span><span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>As with single-agent RL, MARL has a whole new zoo of algorithms and platforms to choose from:</p>
<ul class="simple">
<li><p>MADDPG (openai),
paper: <a class="reference external" href="https://arxiv.org/pdf/1706.02275.pdf">https://arxiv.org/pdf/1706.02275.pdf</a>
original git: <a class="reference external" href="https://github.com/openai/maddpg">https://github.com/openai/maddpg</a>
pytorch implementation: <a class="reference external" href="https://github.com/shariqiqbal2810/maddpg-pytorch">https://github.com/shariqiqbal2810/maddpg-pytorch</a></p></li>
<li><p>MATD3,
paper: <a class="reference external" href="https://arxiv.org/abs/1910.01465">https://arxiv.org/abs/1910.01465</a>
author’s git: <a class="reference external" href="https://github.com/JohannesAck/MATD3implementation">https://github.com/JohannesAck/MATD3implementation</a></p></li>
</ul>
<ul class="simple">
<li><p>QMIX: <a class="reference external" href="https://arxiv.org/abs/1803.11485">https://arxiv.org/abs/1803.11485</a></p></li>
<li><p>MAVEN: <a class="reference external" href="https://arxiv.org/abs/1910.07483">https://arxiv.org/abs/1910.07483</a></p></li>
<li><p>QTRAN: <a class="reference external" href="https://arxiv.org/abs/1905.05408">https://arxiv.org/abs/1905.05408</a></p></li>
<li><p>VDN: <a class="reference external" href="https://arxiv.org/abs/1706.05296">https://arxiv.org/abs/1706.05296</a></p></li>
<li><p>SA-MATD3: <a class="reference external" href="https://arxiv.org/abs/2107.00284">https://arxiv.org/abs/2107.00284</a></p></li>
<li><p>(add-on mod: UPDeT <a class="reference external" href="https://arxiv.org/abs/2101.08001">https://arxiv.org/abs/2101.08001</a>)</p></li>
</ul>
<p>Literally, a zoo:</p>
<p>PettingZoo - the Gym for MA
<a class="reference external" href="https://www.pettingzoo.ml/">https://www.pettingzoo.ml/</a> - <a class="reference external" href="https://github.com/Farama-Foundation/PettingZoo">https://github.com/Farama-Foundation/PettingZoo</a></p>
<p>Because so much research is ongoing, we mainly present some noteworthy resources below, with no value judgement as to what will work best for you.  As you’ll likely see quickly, there’s a lot of overlap, but there’s also a lot of different ways to approach a similar problem.</p>
<p>MAgent
paper: <a class="reference external" href="https://arxiv.org/abs/1712.00600">https://arxiv.org/abs/1712.00600</a>
associated info and code: <a class="reference external" href="https://github.com/geek-ai/MAgent">https://github.com/geek-ai/MAgent</a></p>
<p>Oxford comp-sci MA-RL framework: pyMarl
<a class="reference external" href="https://github.com/oxwhirl/pymarl">https://github.com/oxwhirl/pymarl</a>
and associated (starcraft2-based) <a class="reference external" href="https://github.com/oxwhirl/smac">https://github.com/oxwhirl/smac</a>
(one, of many, interesting takes on the challenge: <a class="reference external" href="https://arxiv.org/abs/2011.09533">https://arxiv.org/abs/2011.09533</a>)</p>
<p>Rl_games
<a class="reference external" href="https://github.com/Denys88/rl_games">https://github.com/Denys88/rl_games</a></p>
<p>Nvidia Isaac Gym
<a class="reference external" href="https://developer.nvidia.com/blog/introducing-isaac-gym-rl-for-robotics/">https://developer.nvidia.com/blog/introducing-isaac-gym-rl-for-robotics/</a>
<a class="reference external" href="https://arxiv.org/abs/2109.11978">https://arxiv.org/abs/2109.11978</a>
<a class="reference external" href="https://github.com/NVIDIA-Omniverse/IsaacGymEnvs">https://github.com/NVIDIA-Omniverse/IsaacGymEnvs</a></p>
<p>Army MARL with centralized training
<a class="reference external" href="https://www.army.mil/article/247261/army_researchers_develop_innovative_framework_for_training_ai">https://www.army.mil/article/247261/army_researchers_develop_innovative_framework_for_training_ai</a></p>
<p>Salesforce Warp-drive
<a class="reference external" href="https://github.com/salesforce/warp-drive">https://github.com/salesforce/warp-drive</a></p>
<p>InstaDeep MAVA
<a class="reference external" href="https://www.instadeep.com/2021/07/mava-a-new-framework-for-distributed-multi-agent-reinforcement-learning/">https://www.instadeep.com/2021/07/mava-a-new-framework-for-distributed-multi-agent-reinforcement-learning/</a>  git: <a class="reference external" href="https://github.com/instadeepai/Mava">https://github.com/instadeepai/Mava</a></p>
<p>DeepMind’s Acme
<a class="reference external" href="https://deepmind.com/research/publications/2020/Acme">https://deepmind.com/research/publications/2020/Acme</a>  git: <a class="reference external" href="https://github.com/deepmind/acme">https://github.com/deepmind/acme</a></p>
<p>Berkeley/Ray-RLlib: <a class="reference external" href="https://bair.berkeley.edu/blog/2018/12/12/rllib/">https://bair.berkeley.edu/blog/2018/12/12/rllib/</a> git: <a class="reference external" href="https://github.com/ray-project/ray">https://github.com/ray-project/ray</a> RLlib docs: <a class="reference external" href="https://docs.ray.io/en/master/rllib/index.html">https://docs.ray.io/en/master/rllib/index.html</a></p>
<p>Intel’s MERL: <a class="reference external" href="https://www.intel.com/content/www/us/en/artificial-intelligence/posts/introducing-merl.html">https://www.intel.com/content/www/us/en/artificial-intelligence/posts/introducing-merl.html</a></p>
<p>JORLD from Kakao Enterprise: <a class="reference external" href="https://www.reddit.com/r/reinforcementlearning/comments/qp9aj5/jorldy_opensource_reinforcement_learning_framework/">https://www.reddit.com/r/reinforcementlearning/comments/qp9aj5/jorldy_opensource_reinforcement_learning_framework/</a> git: <a class="reference external" href="https://github.com/kakaoenterprise/JORLDY">https://github.com/kakaoenterprise/JORLDY</a></p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Course_Material"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="Day4_PartB.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Day 4, Part B: Creating Custom Environments</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="Day5_PartB.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Day 5, Part B: Using Your Trained Policy</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By tonyfast<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>